{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "import fblearner.flow.api as flow\n",
    "from pvc import Dataset, query, preview, load, upload\n",
    "from pvc.operators.dataset.hivedataset import HiveDataset\n",
    "from fblearner.flow.api import types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NAMESPACE = 'instagram'\n",
    "VALIDATION_RESULT_HIVE_TABLE = 'fips_prediction_validation_result'\n",
    "PREDICTION_RESULT_HIVE_TABLE = 'fips_prediction_prediction_result'\n",
    "PERDICTION_PERCENTILES_HIVE_TABLE = 'fips_prediction_prediction_percentiles'\n",
    "NUMERICAL_FEATURE_AGGREGATION_TABLE = 'fips_prediction_numerical_feature_agg'\n",
    "CATEGORICAL_FEATURE_AGGREGATION_TABLE = 'fips_prediction_categorical_feature_agg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_jhu = pd.read_csv(f\"data/us/aggregate_jhu.csv\")\n",
    "\n",
    "# Get rid of the aggregate country data\n",
    "df_jhu = df_jhu.drop([0])\n",
    "df_jhu['FIPS'] = df_jhu['FIPS'].map(lambda f : str(f))\n",
    "\n",
    "def alter(fips):\n",
    "    if len(fips) == 4:\n",
    "        return '0' + fips\n",
    "    return fips\n",
    "df_jhu['FIPS'] = df_jhu['FIPS'].map(alter)\n",
    "df_jhu = df_jhu.set_index('FIPS')\n",
    "df_jhu['fips'] = df_jhu.index.map(lambda s : int(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['POP_ESTIMATE_2018', 'Area in square miles - Land area', 'Density per square mile of land area - Population', 'Total_Male', 'Total_Female', 'Total_age0to17', \n",
    "            'Total_age18to64','Total_age65plus', 'Active Physicians per 100000 Population 2018 (AAMC)', 'Active General Surgeons per 100000 Population 2018 (AAMC)',\n",
    "           'Non-profit hospital beds per 1000 people (2019)', 'Employed_2018', 'Unemployment_rate_2018'\n",
    "           , 'Total hospital beds per 1000 people (2019)', 'Total nurse practitioners (2019)',\n",
    "           'Total Hospitals (2019)','fips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.dataresource+json": {
       "data": [
        {
         "01003": 218022,
         "index": "POP_ESTIMATE_2018"
        },
        {
         "01003": 1589.78,
         "index": "Area in square miles - Land area"
        },
        {
         "01003": 114.6,
         "index": "Density per square mile of land area - Population"
        },
        {
         "01003": 105657,
         "index": "Total_Male"
        },
        {
         "01003": 112365,
         "index": "Total_Female"
        },
        {
         "01003": 47110,
         "index": "Total_age0to17"
        },
        {
         "01003": 126341,
         "index": "Total_age18to64"
        },
        {
         "01003": 44571,
         "index": "Total_age65plus"
        },
        {
         "01003": 217.1,
         "index": "Active Physicians per 100000 Population 2018 (AAMC)"
        },
        {
         "01003": 7.6,
         "index": "Active General Surgeons per 100000 Population 2018 (AAMC)"
        },
        {
         "01003": 0.8,
         "index": "Non-profit hospital beds per 1000 people (2019)"
        },
        {
         "01003": 90456,
         "index": "Employed_2018"
        },
        {
         "01003": 3.6,
         "index": "Unemployment_rate_2018"
        },
        {
         "01003": 3.1,
         "index": "Total hospital beds per 1000 people (2019)"
        },
        {
         "01003": 113.1621137,
         "index": "Total nurse practitioners (2019)"
        },
        {
         "01003": 4.505074295,
         "index": "Total Hospitals (2019)"
        },
        {
         "01003": 1003,
         "index": "fips"
        }
       ],
       "schema": {
        "fields": [
         {
          "name": "index",
          "type": "string"
         },
         {
          "name": "01003",
          "type": "number"
         }
        ],
        "pandas_version": "0.20.0",
        "primaryKey": [
         "index"
        ]
       }
      },
      "text/plain": [
       "POP_ESTIMATE_2018                                            218022.000000\n",
       "Area in square miles - Land area                               1589.780000\n",
       "Density per square mile of land area - Population               114.600000\n",
       "Total_Male                                                   105657.000000\n",
       "Total_Female                                                 112365.000000\n",
       "Total_age0to17                                                47110.000000\n",
       "Total_age18to64                                              126341.000000\n",
       "Total_age65plus                                               44571.000000\n",
       "Active Physicians per 100000 Population 2018 (AAMC)             217.100000\n",
       "Active General Surgeons per 100000 Population 2018 (AAMC)         7.600000\n",
       "Non-profit hospital beds per 1000 people (2019)                   0.800000\n",
       "Employed_2018                                                 90456.000000\n",
       "Unemployment_rate_2018                                            3.600000\n",
       "Total hospital beds per 1000 people (2019)                        3.100000\n",
       "Total nurse practitioners (2019)                                113.162114\n",
       "Total Hospitals (2019)                                            4.505074\n",
       "fips                                                           1003.000000\n",
       "Name: 01003, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "bento_obj_id": "140171078378896"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_jhu[features]\n",
    "df.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df.fips % 1000 != 0] # remove aggregate states\n",
    "# df = df[df.State != 'PR']   # peurto rico has some weird data...\n",
    "df = df[df.POP_ESTIMATE_2018 > 1000] # restrict to large counties since getting lots of data is difficult\n",
    "\n",
    "# fill out missing data\n",
    "df.at['02158', 'Area in square miles - Land area'] = 19673\n",
    "df.at['02158', 'Density per square mile of land area - Population'] = 0.44\n",
    "df.at['46102', 'Area in square miles - Land area'] = 2097\n",
    "df.at['46102', 'Density per square mile of land area - Population'] = 6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_df = pd.melt(df, id_vars=['fips'], value_vars=features[:-1], \\\n",
    "                 var_name='feature_name', value_name='feature_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gbdt_train(ds, model_config):\n",
    "    partition_dict = {'ds': ds}\n",
    "    if model_config.training_set_partition is not None:\n",
    "        partition_dict.update(model_config.training_set_partition)\n",
    "    train_gbdt_input = {\n",
    "        'feature_ds':\n",
    "            ds - timedelta(days=model_config.prediction_timespan),\n",
    "        'label_data': pcvdataset()(Dataset)(\n",
    "            table=model_config.training_set_table,\n",
    "            partition=partition_dict),\n",
    "        'training_validation_split':\n",
    "            model_config.get('training_validation_split'),\n",
    "        'training_negative_sampling_rate':\n",
    "            model_config.get('training_negative_sampling_rate'),\n",
    "    }\n",
    "    train_gbdt_input = dict(\n",
    "        (k, v) for k, v in train_gbdt_input.items() if v is not None)\n",
    "    workflow_metadata = WorkflowRunMetadataMutation(\n",
    "        name='GBDT Training, {0}'.format(\n",
    "            model_config.model_name))\n",
    "    workflow_output = RunWorkflowOperator(\n",
    "        TrainGbdtModelWorkflow,\n",
    "        train_gbdt_input,\n",
    "        metadata=workflow_metadata,\n",
    "        memoize=True,\n",
    "        version=\"v0\")\n",
    "    return workflow_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gbdt_predict(ds, model_config, training_output):\n",
    "    partition_dict = {'ds': ds}\n",
    "    if model_config.predicting_set_partition is not None:\n",
    "        partition_dict.update(model_config.predicting_set_partition)\n",
    "    predict_gbdt_input = {\n",
    "        'feature_ds': ds,\n",
    "        'user_set': pcvdataset()(Dataset)(\n",
    "                table=model_config.predicting_set_table,\n",
    "                partition=partition_dict),\n",
    "        'gbdt_model': training_output.model.path,\n",
    "    }\n",
    "    workflow_metadata = WorkflowRunMetadataMutation(\n",
    "        name='GBDT Training, {0}'.format(\n",
    "            model_config.model_name))\n",
    "    workflow_output = RunWorkflowOperator(\n",
    "        PredictWithGbdtModelWorkflow,\n",
    "        predict_gbdt_input,\n",
    "        metadata=workflow_metadata,\n",
    "        memoize=True,\n",
    "        version=\"v0\")\n",
    "    return workflow_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gbdt_validate(ds, model_config, training_output):\n",
    "    predict_gbdt_input = {\n",
    "        'feature_ds': ds - timedelta(days=model_config.prediction_timespan),\n",
    "        'user_set': training_output.validation_set,\n",
    "        'gbdt_model': training_output.model.path,\n",
    "    }\n",
    "    workflow_metadata = WorkflowRunMetadataMutation(\n",
    "        name='GBDT Evaluating, {0}'.format(\n",
    "            model_config.model_name))\n",
    "    workflow_output = RunWorkflowOperator(\n",
    "        PredictWithGbdtModelWorkflow,\n",
    "        predict_gbdt_input,\n",
    "        metadata=workflow_metadata,\n",
    "        memoize=True,\n",
    "        version=\"v0\")\n",
    "    return workflow_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@flow.flow_async()\n",
    "@flow.typed()\n",
    "def upload_features_df_to_hive(\n",
    "    features_df: pd.DataFrame,\n",
    ") -> HiveDataset:\n",
    "    dataset = upload(\n",
    "    schema=types.STRUCT(\n",
    "        ('number', types.PARTITION),\n",
    "        ('fips', types.INT),\n",
    "        ('feature_name', types.TEXT),\n",
    "        ('feature_value', types.FLOAT),\n",
    "    ),\n",
    "    dataframe=features_df,\n",
    "    lines_delimiter=r\"\\n\",\n",
    "    fields_delimiter=r\"\\t\",\n",
    "    collection_items_delimiter=\",\",\n",
    "    map_keys_delimiter=\":\",\n",
    "    namespace='instagram',\n",
    "#     tablename='fips_feature_names_and_values',\n",
    "    partition=dict(number=1),\n",
    "    retention=180,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# upload_features_df_to_hive(features_df) # partition into fips so notebook doesn't crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nyt_us_counties_df = pd.read_csv('data/us/covid/nyt_us_counties.csv', sep=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gets list of all fips numbers\n",
    "def get_fips():\n",
    "    Y = pd.read_csv(f\"data/us/covid/deaths.csv\")\n",
    "    return set(Y.countyFIPS.values)\n",
    "\n",
    "def get_date(datestr, formatstr='%Y-%m-%d'):\n",
    "    return datetime.strptime(datestr, formatstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CumDeathCounter():\n",
    "    def __init__(self):\n",
    "        self.cum_deaths = pd.read_csv(f\"data/us/covid/deaths.csv\")\n",
    "        self.cum_deaths = self.cum_deaths.iloc[1:]\n",
    "        fips_list = self.cum_deaths.countyFIPS.values\n",
    "        \n",
    "        self.cache = {}\n",
    "        for fips in fips_list:\n",
    "            self.cache[fips] = self.get_cum_deaths(fips)\n",
    "            \n",
    "    def get_cum_deaths(self, fips, clip_zeros=False):\n",
    "        idx = self.cum_deaths.index[self.cum_deaths['countyFIPS'] == fips].values[0]\n",
    "        county_deaths = self.cum_deaths.loc[self.cum_deaths['countyFIPS'] == fips]\n",
    "        dates = pd.to_datetime(county_deaths.columns[4:].values).map(lambda dt : str(dt))\n",
    "#         X = np.array([(get_date(d[:10]) - get_date('2020-01-01')).days for d in dates])\n",
    "        X = np.array([get_date(d[:10]).date().isoformat() for d in dates])\n",
    "        y = []\n",
    "        for i in range(4, len(county_deaths.columns)):\n",
    "            y.append(county_deaths.loc[idx,county_deaths.columns[i]])\n",
    "        if not clip_zeros:\n",
    "            return X, y\n",
    "        for i in range(len(y)):\n",
    "            if y[i] != 0:\n",
    "                return X[i:], y[i:]\n",
    "            \n",
    "    def getY(self, fips):\n",
    "        return self.cache[fips]\n",
    "    \n",
    "    def getCache(self):\n",
    "        return self.cache\n",
    "    \n",
    "class CumCaseCounter():\n",
    "    def __init__(self):\n",
    "        self.cum_cases = pd.read_csv(f\"data/us/covid/confirmed_cases.csv\")\n",
    "        self.cum_cases = self.cum_cases.iloc[1:]\n",
    "        self.cum_cases = self.cum_cases.iloc[:, :-1]\n",
    "        \n",
    "        fips_list = self.cum_cases.countyFIPS.values\n",
    "        \n",
    "        self.cache = {}\n",
    "        for fips in fips_list:\n",
    "            self.cache[fips] = self.get_cum_cases(fips)\n",
    "        \n",
    "    def get_cum_cases(self, fips,clip_zeros=False):\n",
    "        idx = self.cum_cases.index[self.cum_cases['countyFIPS'] == fips].values[0]\n",
    "        county_cases = self.cum_cases.loc[self.cum_cases['countyFIPS'] == fips]\n",
    "        dates = pd.to_datetime(county_cases.columns[4:].values).map(lambda dt : str(dt))\n",
    "#         X = np.array([(get_date(d[:10]) - get_date('2020-01-01')).days for d in dates])\n",
    "        X = np.array([get_date(d[:10]).date().isoformat() for d in dates])\n",
    "        y = []\n",
    "        for i in range(4, len(county_cases.columns)):\n",
    "            y.append(county_cases.loc[idx,county_cases.columns[i]])\n",
    "        if not clip_zeros:\n",
    "            return X, y\n",
    "        for i in range(len(y)):\n",
    "            if y[i] != 0:\n",
    "                return X[i:], y[i:]\n",
    "            \n",
    "    def getY(self, fips):\n",
    "        return self.cache[fips]\n",
    "    \n",
    "    def getCache(self):\n",
    "        return self.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cum_death_counter = CumDeathCounter()\n",
    "cum_case_counter = CumCaseCounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_deaths = cum_death_counter.getCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_deaths_df = pd.DataFrame(cum_deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_deaths_df_transposed = cum_deaths_df.T.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index                                                  0  \\\n",
      "0      1001  [2020-01-22, 2020-01-23, 2020-01-24, 2020-01-2...   \n",
      "1      1003  [2020-01-22, 2020-01-23, 2020-01-24, 2020-01-2...   \n",
      "2      1005  [2020-01-22, 2020-01-23, 2020-01-24, 2020-01-2...   \n",
      "3      1007  [2020-01-22, 2020-01-23, 2020-01-24, 2020-01-2...   \n",
      "4      1009  [2020-01-22, 2020-01-23, 2020-01-24, 2020-01-2...   \n",
      "...     ...                                                ...   \n",
      "3141  56037  [2020-01-22, 2020-01-23, 2020-01-24, 2020-01-2...   \n",
      "3142  56039  [2020-01-22, 2020-01-23, 2020-01-24, 2020-01-2...   \n",
      "3143  56041  [2020-01-22, 2020-01-23, 2020-01-24, 2020-01-2...   \n",
      "3144  56043  [2020-01-22, 2020-01-23, 2020-01-24, 2020-01-2...   \n",
      "3145  56045  [2020-01-22, 2020-01-23, 2020-01-24, 2020-01-2...   \n",
      "\n",
      "                                                      1  \n",
      "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "...                                                 ...  \n",
      "3141  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3142  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3143  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3144  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3145  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "\n",
      "[3146 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(cum_deaths_df_transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cum_deaths_df_transposed = cum_deaths_df_transposed.rename(columns={'index':'fips', 0:'ds', 1:'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode(df, lst_cols, fill_value='', preserve_index=False):\n",
    "    # make sure `lst_cols` is list-alike\n",
    "    if (lst_cols is not None\n",
    "        and len(lst_cols) > 0\n",
    "        and not isinstance(lst_cols, (list, tuple, np.ndarray, pd.Series))):\n",
    "        lst_cols = [lst_cols]\n",
    "    # all columns except `lst_cols`\n",
    "    idx_cols = df.columns.difference(lst_cols)\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_cols[0]].str.len()\n",
    "    # preserve original index values    \n",
    "    idx = np.repeat(df.index.values, lens)\n",
    "    # create \"exploded\" DF\n",
    "    res = (pd.DataFrame({\n",
    "                col:np.repeat(df[col].values, lens)\n",
    "                for col in idx_cols},\n",
    "                index=idx)\n",
    "             .assign(**{col:np.concatenate(df.loc[lens>0, col].values)\n",
    "                            for col in lst_cols}))\n",
    "    # append those rows that have empty lists\n",
    "    if (lens == 0).any():\n",
    "        # at least one list in cells is empty\n",
    "        res = (res.append(df.loc[lens==0, idx_cols], sort=False)\n",
    "                  .fillna(fill_value))\n",
    "    # revert the original index order\n",
    "    res = res.sort_index()\n",
    "    # reset index if requested\n",
    "    if not preserve_index:        \n",
    "        res = res.reset_index(drop=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_labels_df = explode(cum_deaths_df_transposed, ['ds', 'label'], fill_value='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.dataresource+json": {
       "data": [
        {
         "ds": "2020-01-22",
         "fips": 1001,
         "index": 0,
         "label": 0
        },
        {
         "ds": "2020-01-23",
         "fips": 1001,
         "index": 1,
         "label": 0
        },
        {
         "ds": "2020-01-24",
         "fips": 1001,
         "index": 2,
         "label": 0
        },
        {
         "ds": "2020-01-25",
         "fips": 1001,
         "index": 3,
         "label": 0
        },
        {
         "ds": "2020-01-26",
         "fips": 1001,
         "index": 4,
         "label": 0
        },
        {
         "ds": "2020-01-27",
         "fips": 1001,
         "index": 5,
         "label": 0
        },
        {
         "ds": "2020-01-28",
         "fips": 1001,
         "index": 6,
         "label": 0
        },
        {
         "ds": "2020-01-29",
         "fips": 1001,
         "index": 7,
         "label": 0
        },
        {
         "ds": "2020-01-30",
         "fips": 1001,
         "index": 8,
         "label": 0
        },
        {
         "ds": "2020-01-31",
         "fips": 1001,
         "index": 9,
         "label": 0
        },
        {
         "ds": "2020-02-01",
         "fips": 1001,
         "index": 10,
         "label": 0
        },
        {
         "ds": "2020-02-02",
         "fips": 1001,
         "index": 11,
         "label": 0
        },
        {
         "ds": "2020-02-03",
         "fips": 1001,
         "index": 12,
         "label": 0
        },
        {
         "ds": "2020-02-04",
         "fips": 1001,
         "index": 13,
         "label": 0
        },
        {
         "ds": "2020-02-05",
         "fips": 1001,
         "index": 14,
         "label": 0
        },
        {
         "ds": "2020-02-06",
         "fips": 1001,
         "index": 15,
         "label": 0
        },
        {
         "ds": "2020-02-07",
         "fips": 1001,
         "index": 16,
         "label": 0
        },
        {
         "ds": "2020-02-08",
         "fips": 1001,
         "index": 17,
         "label": 0
        },
        {
         "ds": "2020-02-09",
         "fips": 1001,
         "index": 18,
         "label": 0
        },
        {
         "ds": "2020-02-10",
         "fips": 1001,
         "index": 19,
         "label": 0
        },
        {
         "ds": "2020-02-11",
         "fips": 1001,
         "index": 20,
         "label": 0
        },
        {
         "ds": "2020-02-12",
         "fips": 1001,
         "index": 21,
         "label": 0
        },
        {
         "ds": "2020-02-13",
         "fips": 1001,
         "index": 22,
         "label": 0
        },
        {
         "ds": "2020-02-14",
         "fips": 1001,
         "index": 23,
         "label": 0
        },
        {
         "ds": "2020-02-15",
         "fips": 1001,
         "index": 24,
         "label": 0
        },
        {
         "ds": "2020-02-16",
         "fips": 1001,
         "index": 25,
         "label": 0
        },
        {
         "ds": "2020-02-17",
         "fips": 1001,
         "index": 26,
         "label": 0
        },
        {
         "ds": "2020-02-18",
         "fips": 1001,
         "index": 27,
         "label": 0
        },
        {
         "ds": "2020-02-19",
         "fips": 1001,
         "index": 28,
         "label": 0
        },
        {
         "ds": "2020-02-20",
         "fips": 1001,
         "index": 29,
         "label": 0
        },
        {
         "ds": "2020-02-21",
         "fips": 1001,
         "index": 30,
         "label": 0
        },
        {
         "ds": "2020-02-22",
         "fips": 1001,
         "index": 31,
         "label": 0
        },
        {
         "ds": "2020-02-23",
         "fips": 1001,
         "index": 32,
         "label": 0
        },
        {
         "ds": "2020-02-24",
         "fips": 1001,
         "index": 33,
         "label": 0
        },
        {
         "ds": "2020-02-25",
         "fips": 1001,
         "index": 34,
         "label": 0
        },
        {
         "ds": "2020-02-26",
         "fips": 1001,
         "index": 35,
         "label": 0
        },
        {
         "ds": "2020-02-27",
         "fips": 1001,
         "index": 36,
         "label": 0
        },
        {
         "ds": "2020-02-28",
         "fips": 1001,
         "index": 37,
         "label": 0
        },
        {
         "ds": "2020-02-29",
         "fips": 1001,
         "index": 38,
         "label": 0
        },
        {
         "ds": "2020-03-01",
         "fips": 1001,
         "index": 39,
         "label": 0
        },
        {
         "ds": "2020-03-02",
         "fips": 1001,
         "index": 40,
         "label": 0
        },
        {
         "ds": "2020-03-03",
         "fips": 1001,
         "index": 41,
         "label": 0
        },
        {
         "ds": "2020-03-04",
         "fips": 1001,
         "index": 42,
         "label": 0
        },
        {
         "ds": "2020-03-05",
         "fips": 1001,
         "index": 43,
         "label": 0
        },
        {
         "ds": "2020-03-06",
         "fips": 1001,
         "index": 44,
         "label": 0
        },
        {
         "ds": "2020-03-07",
         "fips": 1001,
         "index": 45,
         "label": 0
        },
        {
         "ds": "2020-03-08",
         "fips": 1001,
         "index": 46,
         "label": 0
        },
        {
         "ds": "2020-03-09",
         "fips": 1001,
         "index": 47,
         "label": 0
        },
        {
         "ds": "2020-03-10",
         "fips": 1001,
         "index": 48,
         "label": 0
        },
        {
         "ds": "2020-03-11",
         "fips": 1001,
         "index": 49,
         "label": 0
        },
        {
         "ds": "2020-03-12",
         "fips": 1001,
         "index": 50,
         "label": 0
        },
        {
         "ds": "2020-03-13",
         "fips": 1001,
         "index": 51,
         "label": 0
        },
        {
         "ds": "2020-03-14",
         "fips": 1001,
         "index": 52,
         "label": 0
        },
        {
         "ds": "2020-03-15",
         "fips": 1001,
         "index": 53,
         "label": 0
        },
        {
         "ds": "2020-03-16",
         "fips": 1001,
         "index": 54,
         "label": 0
        },
        {
         "ds": "2020-03-17",
         "fips": 1001,
         "index": 55,
         "label": 0
        },
        {
         "ds": "2020-03-18",
         "fips": 1001,
         "index": 56,
         "label": 0
        },
        {
         "ds": "2020-03-19",
         "fips": 1001,
         "index": 57,
         "label": 0
        },
        {
         "ds": "2020-03-20",
         "fips": 1001,
         "index": 58,
         "label": 0
        },
        {
         "ds": "2020-03-21",
         "fips": 1001,
         "index": 59,
         "label": 0
        }
       ],
       "schema": {
        "fields": [
         {
          "name": "index",
          "type": "integer"
         },
         {
          "name": "fips",
          "type": "integer"
         },
         {
          "name": "ds",
          "type": "string"
         },
         {
          "name": "label",
          "type": "integer"
         }
        ],
        "pandas_version": "0.20.0",
        "primaryKey": [
         "index"
        ]
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>ds</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279989</th>\n",
       "      <td>56045</td>\n",
       "      <td>2020-04-15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279990</th>\n",
       "      <td>56045</td>\n",
       "      <td>2020-04-16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279991</th>\n",
       "      <td>56045</td>\n",
       "      <td>2020-04-17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279992</th>\n",
       "      <td>56045</td>\n",
       "      <td>2020-04-18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279993</th>\n",
       "      <td>56045</td>\n",
       "      <td>2020-04-19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279994 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         fips          ds  label\n",
       "0        1001  2020-01-22      0\n",
       "1        1001  2020-01-23      0\n",
       "2        1001  2020-01-24      0\n",
       "3        1001  2020-01-25      0\n",
       "4        1001  2020-01-26      0\n",
       "...       ...         ...    ...\n",
       "279989  56045  2020-04-15      0\n",
       "279990  56045  2020-04-16      0\n",
       "279991  56045  2020-04-17      0\n",
       "279992  56045  2020-04-18      0\n",
       "279993  56045  2020-04-19      0\n",
       "\n",
       "[279994 rows x 3 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {
      "bento_obj_id": "140170665635472"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "death_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @flow.flow_async()\n",
    "# @flow.typed()\n",
    "def upload_labels_df_to_hive(\n",
    "    labels_df: pd.DataFrame,\n",
    "    ds: str,\n",
    ") -> HiveDataset:\n",
    "    dataset = upload(\n",
    "    schema=types.STRUCT(\n",
    "        ('ds', types.PARTITION),\n",
    "        ('number', types.PARTITION),\n",
    "        ('fips', types.INT),\n",
    "#         ('features', types.TEXT),\n",
    "        ('label', types.FLOAT),\n",
    "    ),\n",
    "    dataframe=labels_df,\n",
    "    lines_delimiter=r\"\\n\",\n",
    "    fields_delimiter=r\"\\t\",\n",
    "    collection_items_delimiter=\",\",\n",
    "    map_keys_delimiter=\":\",\n",
    "    namespace='instagram',\n",
    "#     tablename='fips_features_labels_training_data',\n",
    "    partition=dict(ds=ds, number=1),\n",
    "    retention=180,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "name": "upload",
       "operator_id": "610b5f1e-8b8b-4ad7-9eb3-30d81adc68b6",
       "output": [],
       "status": "scheduled"
      },
      "text/html": [
       "<div class=\"operator_area operator_running operator_closed\" data-bento-flow-uuid=\"610b5f1e-8b8b-4ad7-9eb3-30d81adc68b6\" data-bento-materialized-id=\"b01e9aa4-57a4-4d46-ac5b-5a665c2c7112\">\n",
       "    <div class=\"operator_header\">\n",
       "        <span class=\"expand\"></span> \n",
       "        Operator <strong>upload</strong>\n",
       "        <span class=\"operator_panel\">\n",
       "            <a href=\"javascript:void()\" class=\"kill_operator\">☠</a>\n",
       "            <span class=\"operator_status\"></span>\n",
       "        </span>\n",
       "    </div>\n",
       "    <div class=\"operator_output\" style=\"\"><span class=\"stderr\">I0423 074309.436 operatorcall.py:324] Resolved inputs for operator upload in 0.006s<br></span><span class=\"stderr\">I0423 074312.479 operatorcall.py:336] Saved resolved inputs for operator upload in 3.042s<br></span><span class=\"stderr\">I0423 074312.482 memoized.py:317] Memoization [upload]: Arguments used for hash calculation: namespace, schema, path, dataframe, tablename, pool, partition, pre, lines_delimiter, fields_delimiter, collection_items_delimiter, map_keys_delimiter, retention, sort_by, cluster_by, buckets, tablename_prefix, _custom_name, no_uii, materialize, should_materialize, execute_locally<br></span><span class=\"stderr\">I0423 074312.482 memoized.py:324] Memoization [upload]: Arguments ignored: N/A<br></span><span class=\"stderr\">W0423 074312.485 memoized.py:236] Function is unmemoizable: Unhashable object &lt;pandas._libs.index.ObjectEngine object at 0x7f7c1d65ffb0&gt; of type &lt;class 'pandas._libs.index.ObjectEngine'&gt;.<br>Traceback (most recent call last):<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 387, in maketuple<br>    enc = (\"dump\", cloudpickle.dumps(obj))<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/cloudpickle/cloudpickle.py\", line 1097, in dumps<br>    cp.dump(obj)<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/cloudpickle/cloudpickle.py\", line 357, in dump<br>    return Pickler.dump(self, obj)<br>  File \"/usr/local/fbcode/platform007/lib/python3.7/pickle.py\", line 437, in dump<br>    self.save(obj)<br>  File \"/usr/local/fbcode/platform007/lib/python3.7/pickle.py\", line 549, in save<br>    self.save_reduce(obj=obj, *rv)<br>  File \"/usr/local/fbcode/platform007/lib/python3.7/pickle.py\", line 662, in save_reduce<br>    save(state)<br>  File \"/usr/local/fbcode/platform007/lib/python3.7/pickle.py\", line 504, in save<br>    f(self, obj) # Call unbound method with explicit self<br>  File \"/usr/local/fbcode/platform007/lib/python3.7/pickle.py\", line 789, in save_tuple<br>    save(element)<br>  File \"/usr/local/fbcode/platform007/lib/python3.7/pickle.py\", line 524, in save<br>    rv = reduce(self.proto)<br>  File \"stringsource\", line 2, in pandas._libs.hashtable.PyObjectHashTable.__reduce_cython__<br>TypeError: self.table cannot be converted to a Python object for pickling<br><br>During handling of the above exception, another exception occurred:<br><br>Traceback (most recent call last):<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/decorators_lib/memoized.py\", line 226, in make_memoization_hash<br>    use_cache=compute_hash_with_object_cache,<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 397, in make_hashable<br>    maketuple(obj_1)<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 180, in maketuple<br>    enc = (\"tuple\", tuple((maketuple(x) for x in obj)))<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 180, in &lt;genexpr&gt;<br>    enc = (\"tuple\", tuple((maketuple(x) for x in obj)))<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 187, in maketuple<br>    for key, value in sorted(obj.items(), key=KeyFunc)<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 189, in &lt;genexpr&gt;<br>    isinstance(key, string_types) and key.startswith(\"_cache_\")<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 384, in maketuple<br>    enc += (maketuple(obj.__dict__),)<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 187, in maketuple<br>    for key, value in sorted(obj.items(), key=KeyFunc)<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 189, in &lt;genexpr&gt;<br>    isinstance(key, string_types) and key.startswith(\"_cache_\")<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 369, in maketuple<br>    for value in allslots<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 195, in maketuple<br>    enc = (\"list\", tuple((maketuple(x) for x in obj)))<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 195, in &lt;genexpr&gt;<br>    enc = (\"list\", tuple((maketuple(x) for x in obj)))<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 195, in maketuple<br>    enc = (\"list\", tuple((maketuple(x) for x in obj)))<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 195, in &lt;genexpr&gt;<br>    enc = (\"list\", tuple((maketuple(x) for x in obj)))<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 384, in maketuple<br>    enc += (maketuple(obj.__dict__),)<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 187, in maketuple<br>    for key, value in sorted(obj.items(), key=KeyFunc)<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 189, in &lt;genexpr&gt;<br>    isinstance(key, string_types) and key.startswith(\"_cache_\")<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 187, in maketuple<br>    for key, value in sorted(obj.items(), key=KeyFunc)<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 189, in &lt;genexpr&gt;<br>    isinstance(key, string_types) and key.startswith(\"_cache_\")<br>  File \"/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/fblearner/flow/core/hash.py\", line 390, in maketuple<br>    \"Unhashable object {} of type {}\".format(obj, type(obj))<br>fblearner.flow.util.python_utils.NonRetryableTypeError: Unhashable object &lt;pandas._libs.index.ObjectEngine object at 0x7f7c1d65ffb0&gt; of type &lt;class 'pandas._libs.index.ObjectEngine'&gt;<br></span><span class=\"stderr\">I0423 074312.488 memoized.py:354] Memoization [upload]: hash=None memoizable=False enabled=False<br></span><span class=\"stderr\">I0423 074341.565 plasmajoblauncheroperator.py:194] local_fork: '/usr/local/bin/spark-wrapper --debug run-utility flow-submit  -j '/usr/local/jdk-8u60-64/' -q 'instagram/adhoc' -u 'jsc' -e spark.driver.maxResultSize=8g -e spark.hive.outerjoin.supports.filters=false -e spark.sql.autoShuffledHashJoinThreshold=67108864 -e spark.sql.dynamicJoin=true -e spark.sql.enable.autopartition=true -e spark.sql.estimate.stats=true -e spark.sql.join.preferSortMergeJoin=false -e spark.sql.shuffle.partition.multiplier=0.125 -e spark.sql.shuffle.partitions.max=10000 -e spark.sql.shuffle.partitions.min=997 -e spark.sql.switch.join=true  -C 'org.apache.spark.generichql.GenericHQL' '{\"appname\": \"pvc.spark.instagram\", \"namespace\": \"instagram\", \"queries\": [{\"show\": false, \"splitQueries\": true, \"cache\": false, \"repartition\": 0, \"coalesce\": 0, \"assertCheck\": false, \"alias\": \"\", \"hql\": \"\\n    \\n    CREATE TABLE IF NOT EXISTS tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d (\\n        fips INT, label FLOAT\\n    )\\n    ROW FORMAT DELIMITED\\n    FIELDS TERMINATED BY '\"'\"'\\\\001'\"'\"'\\n    COLLECTION ITEMS TERMINATED BY '\"'\"'\\\\002'\"'\"'\\n    MAP KEYS TERMINATED BY '\"'\"'\\\\003'\"'\"'\\n    LINES TERMINATED BY '\"'\"'\\\\n'\"'\"'\\n    STORED AS TEXTFILE\\n    TBLPROPERTIES ('\"'\"'RETENTION'\"'\"' = '\"'\"'7'\"'\"');\\n\\n    LOAD DATA LOCAL INPATH '\"'\"'/mnt/vol/gfsfblearner-texas2/flow/data/2020-04-23/d0868775-82e9-48d4-99d8-1509d8bf068b'\"'\"'\\n    OVERWRITE INTO TABLE tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;\\n\\n    \\n    CREATE TABLE IF NOT EXISTS tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8 (\\n        fips INT, label FLOAT\\n    )\\n    PARTITIONED BY (ds STRING, number STRING)\\n    \\n    TBLPROPERTIES('\"'\"'RETENTION'\"'\"' = '\"'\"'180'\"'\"');\\n    \\n    \\n    CREATE DEPENDENT TABLE IF NOT EXISTS tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8_signal\\n    REFERS TO TABLE tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8\\n    GROUPED BY (ds);\\n    \\n    \\n    INSERT OVERWRITE TABLE tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8\\n        PARTITION (ds = \\\"2020-04-21\\\", number = \\\"1\\\")\\n        SELECT fips, label FROM tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;;\\n    \\n\\n    DROP TABLE IF EXISTS tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;\\n    \"}]}''.<br></span><span class=\"stderr\">2020-04-23 07:43:42,196 DEBUG: args: {\"appname\": \"pvc.spark.instagram\", \"namespace\": \"instagram\", \"queries\": [{\"show\": false, \"splitQueries\": true, \"cache\": false, \"repartition\": 0, \"coalesce\": 0, \"assertCheck\": false, \"alias\": \"\", \"hql\": \"\\n    \\n    CREATE TABLE IF NOT EXISTS tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d (\\n        fips INT, label FLOAT\\n    )\\n    ROW FORMAT DELIMITED\\n    FIELDS TERMINATED BY '\\\\001'\\n    COLLECTION ITEMS TERMINATED BY '\\\\002'\\n    MAP KEYS TERMINATED BY '\\\\003'\\n    LINES TERMINATED BY '\\\\n'\\n    STORED AS TEXTFILE\\n    TBLPROPERTIES ('RETENTION' = '7');\\n\\n    LOAD DATA LOCAL INPATH '/mnt/vol/gfsfblearner-texas2/flow/data/2020-04-23/d0868775-82e9-48d4-99d8-1509d8bf068b'\\n    OVERWRITE INTO TABLE tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;\\n\\n    \\n    CREATE TABLE IF NOT EXISTS tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8 (\\n        fips INT, label FLOAT\\n    )\\n    PARTITIONED BY (ds STRING, number STRING)\\n    \\n    TBLPROPERTIES('RETENTION' = '180');\\n    \\n    \\n    CREATE DEPENDENT TABLE IF NOT EXISTS tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8_signal\\n    REFERS TO TABLE tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8\\n    GROUPED BY (ds);\\n    \\n    \\n    INSERT OVERWRITE TABLE tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8\\n        PARTITION (ds = \\\"2020-04-21\\\", number = \\\"1\\\")\\n        SELECT fips, label FROM tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;;\\n    \\n\\n    DROP TABLE IF EXISTS tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;\\n    \"}]}<br></span><span class=\"stderr\">2020-04-23 07:43:42,196 INFO: Retrieve spark.app.name = pvc.spark.instagram<br></span><span class=\"stderr\">2020-04-23 07:43:42,196 DEBUG: Using current directory /data/users/jsc/personal/covid<br></span><span class=\"stderr\">2020-04-23 07:43:42,197 INFO: tag for instagram, is_kds=False: phase3<br></span><span class=\"stderr\">2020-04-23 07:43:42,899 INFO: options for namespace instagram: Munch({'spark.cosco.enable.reducer.task.upper.count': '6000', 'spark.fb.only.label.based.scheduling.enabled': 'true', 'spark.fb.only.tetris.enabled.status': 'logonly'})<br></span><span class=\"stderr\">2020-04-23 07:43:44,674 DEBUG: No HIVE_QUERY_SOURCE_JSON, will only apply canaries that don't depend on it<br></span><span class=\"stderr\">2020-04-23 07:43:44,674 DEBUG: Evaluating canary rule: 'Increase driver memory for telemetry pipeline' ...<br></span><span class=\"stderr\">2020-04-23 07:43:44,674 DEBUG: Skipping rule Increase driver memory for telemetry pipeline because of lack of HIVE_QUERY_SOURCE_JSON<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Evaluating canary rule: 'Increase driver memory for telemetry pipeline2' ...<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Skipping rule Increase driver memory for telemetry pipeline2 because of lack of HIVE_QUERY_SOURCE_JSON<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Evaluating canary rule: 'Disable config validation for T57071366' ...<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Skipping rule Disable config validation for T57071366 because of lack of HIVE_QUERY_SOURCE_JSON<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Evaluating canary rule: 'Disable config validation for morse KDS' ...<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Skipping rule Disable config validation for morse KDS because of lack of HIVE_QUERY_SOURCE_JSON<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Evaluating canary rule: 'Disable config validation for morse non-KDS' ...<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Skipping rule Disable config validation for morse non-KDS because of lack of HIVE_QUERY_SOURCE_JSON<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Evaluating canary rule: 'Dynamic speculation for KDS jobs' ...<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Skipping rule Dynamic speculation for KDS jobs because of lack of HIVE_QUERY_SOURCE_JSON<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Evaluating canary rule: 'Increase driver memory for KDS' ...<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Skipping rule Increase driver memory for KDS because of lack of HIVE_QUERY_SOURCE_JSON<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Evaluating canary rule: 'To unblock T63150564' ...<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Skipping rule To unblock T63150564 because of lack of HIVE_QUERY_SOURCE_JSON<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Evaluating canary rule: 'Use 1529 for telemetry' ...<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Skipping rule Use 1529 for telemetry because of lack of HIVE_QUERY_SOURCE_JSON<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Evaluating canary rule: 'Use stable version of spark from 3rd retry onwards' ...<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Skipping rule Use stable version of spark from 3rd retry onwards because of lack of HIVE_QUERY_SOURCE_JSON<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Evaluating canary rule: 'Disable wholestage codegen for coefficient.coefficient3.laser_index_spark (T53487978)' ...<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Skipping rule Disable wholestage codegen for coefficient.coefficient3.laser_index_spark (T53487978) because of lack of HIVE_QUERY_SOURCE_JSON<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Evaluating canary rule: 'Disable Cosco for a pipeline that consistently fails with reducer on-heap OOM' ...<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Skipping rule Disable Cosco for a pipeline that consistently fails with reducer on-heap OOM because of lack of HIVE_QUERY_SOURCE_JSON<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Evaluating canary rule: 'Disable Cosco for a pipeline that consistently fails with reducer on-heap OOM 3' ...<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Skipping rule Disable Cosco for a pipeline that consistently fails with reducer on-heap OOM 3 because of lack of HIVE_QUERY_SOURCE_JSON<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Evaluating canary rule: 'Disable proxyFs for Cosco jobs that fail' ...<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Skipping rule Disable proxyFs for Cosco jobs that fail because of lack of HIVE_QUERY_SOURCE_JSON<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Evaluating canary rule: 'Increase driver memory for adsatlas Hive2FileOperator' ...<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Skipping rule Increase driver memory for adsatlas Hive2FileOperator because of lack of HIVE_QUERY_SOURCE_JSON<br></span><span class=\"stderr\">2020-04-23 07:43:44,675 DEBUG: Evaluating canary rule: 'Run some dataswarm operators in local mode.' ...<br></span><span class=\"stderr\">2020-04-23 07:43:44,676 INFO: Skipping rule `Run some dataswarm operators in local mode.` because the namespace `instagram` does not match `coefficient|commerce|finance|hr|identity|test_namespace|videos`<br></span><span class=\"stderr\">2020-04-23 07:43:44,681 INFO: HIVE_QUERY_SOURCE_JSON is not set!<br></span><span class=\"stderr\">2020-04-23 07:43:44,694 DEBUG: Downloading spark version 1544<br></span><span class=\"stderr\">2020-04-23 07:43:44,695 DEBUG: namespace: instagram<br></span><span class=\"stderr\">2020-04-23 07:43:44,695 DEBUG: queue is : dw-atn-spark1_adhoc<br></span><span class=\"stderr\">2020-04-23 07:43:44,695 INFO: Namespace instagram is not whitelisted for Startup Delay<br></span><span class=\"stderr\">2020-04-23 07:43:44,695 INFO: Applying configs for phase3<br></span><span class=\"stderr\">2020-04-23 07:43:46,483 DEBUG: Creating scuba sample: {\"int\": {\"time\": 1587653026, \"cpu_cores\": 80, \"total_memory_kb\": \"263652028\", \"run_time\": 4}, \"normal\": {\"user\": \"jsc\", \"host\": \"devbig323.ftw3.facebook.com\", \"queue\": \"dw-atn-spark1_adhoc\", \"cluster\": \"dw-atn-spark1\", \"namespace\": \"instagram\", \"datacenter\": \"atn3\", \"tempfs\": \"tempfs://ws.dw.atn3oxygen/spark\", \"SPARK_RELEASE\": \"1544\", \"SPARK_PHASE\": \"phase3\", \"cmd_line\": \"'/usr/local/bin/spark-wrapper' '--debug' 'run-utility' 'flow-submit' '-j' '/usr/local/jdk-8u60-64/' '-q' 'instagram/adhoc' '-u' 'jsc' '-e' 'spark.driver.maxResultSize=8g' '-e' 'spark.hive.outerjoin.supports.filters=false' '-e' 'spark.sql.autoShuffledHashJoinThreshold=67108864' '-e' 'spark.sql.dynamicJoin=true' '-e' 'spark.sql.enable.autopartition=true' '-e' 'spark.sql.estimate.stats=true' '-e' 'spark.sql.join.preferSortMergeJoin=false' '-e' 'spark.sql.shuffle.partition.multiplier=0.125' '-e' 'spark.sql.shuffle.partitions.max=10000' '-e' 'spark.sql.shuffle.partitions.min=997' '-e' 'spark.sql.switch.join=true' '-C' 'org.apache.spark.generichql.GenericHQL' '{\\\"appname\\\": \\\"pvc.spark.instagram\\\", \\\"namespace\\\": \\\"instagram\\\", \\\"queries\\\": [{\\\"show\\\": false, \\\"splitQueries\\\": true, \\\"cache\\\": false, \\\"repartition\\\": 0, \\\"coalesce\\\": 0, \\\"assertCheck\\\": false, \\\"alias\\\": \\\"\\\", \\\"hql\\\": \\\"\\\\n    \\\\n    CREATE TABLE IF NOT EXISTS tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d (\\\\n        fips INT, label FLOAT\\\\n    )\\\\n    ROW FORMAT DELIMITED\\\\n    FIELDS TERMINATED BY '\\\\\\\\001'\\\\n    COLLECTION ITEMS TERMINATED BY '\\\\\\\\002'\\\\n    MAP KEYS TERMINATED BY '\\\\\\\\003'\\\\n    LINES TERMINATED BY '\\\\\\\\n'\\\\n    STORED AS TEXTFILE\\\\n    TBLPROPERTIES ('RETENTION' = '7');\\\\n\\\\n    LOAD DATA LOCAL INPATH '/mnt/vol/gfsfblearner-texas2/flow/data/2020-04-23/d0868775-82e9-48d4-99d8-1509d8bf068b'\\\\n    OVERWRITE INTO TABLE tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;\\\\n\\\\n    \\\\n    CREATE TABLE IF NOT EXISTS tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8 (\\\\n        fips INT, label FLOAT\\\\n    )\\\\n    PARTITIONED BY (ds STRING, number STRING)\\\\n    \\\\n    TBLPROPERTIES('RETENTION' = '180');\\\\n    \\\\n    \\\\n    CREATE DEPENDENT TABLE IF NOT EXISTS tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8_signal\\\\n    REFERS TO TABLE tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8\\\\n    GROUPED BY (ds);\\\\n    \\\\n    \\\\n    INSERT OVERWRITE TABLE tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8\\\\n        PARTITION (ds = \\\\\\\"2020-04-21\\\\\\\", number = \\\\\\\"1\\\\\\\")\\\\n        SELECT fips, label FROM tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;;\\\\n    \\\\n\\\\n    DROP TABLE IF EXISTS tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;\\\\n    \\\"}]}'\", \"spark_utility\": \"dataswarm\", \"r_release\": \"3.6.1\", \"spark_wrapper_version\": \"135\", \"kernel\": \"4.16.18-196_fbk21_5305_g20f611be3d50\", \"os_release\": \"CentOS Linux release 7.6.1810 (Core) \", \"python_path\": \"/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/python/plasma/:/mnt/xarfuse/uid-117004/7fcde8e6-ns-4026531840\", \"working_dir\": \"/data/users/jsc/personal/covid\", \"chronos_job_instance_id\": null, \"chronos_job_name\": null, \"operation\": \"START\", \"invalid_fbpkg\": \"null\", \"classification\": null, \"exit_code\": \"0\", \"wrapper_exception\": null, \"hive_query_source_json\": null, \"original_spark_release\": \"1544\", \"udfs_version\": \"652\", \"udfs_phase\": \"phase3\", \"previous_runs_failure_info\": \"{}\", \"overridden_application_configs\": \"{\\\"spark.driver.maxResultSize\\\": \\\"8g\\\", \\\"spark.hive.outerjoin.supports.filters\\\": \\\"false\\\", \\\"spark.sql.autoShuffledHashJoinThreshold\\\": \\\"67108864\\\", \\\"spark.sql.dynamicJoin\\\": \\\"true\\\", \\\"spark.sql.enable.autopartition\\\": \\\"true\\\", \\\"spark.sql.estimate.stats\\\": \\\"true\\\", \\\"spark.sql.join.preferSortMergeJoin\\\": \\\"false\\\", \\\"spark.sql.shuffle.partition.multiplier\\\": \\\"0.125\\\", \\\"spark.sql.shuffle.partitions.max\\\": \\\"10000\\\", \\\"spark.sql.shuffle.partitions.min\\\": \\\"997\\\", \\\"spark.sql.switch.join\\\": \\\"true\\\"}\", \"configs_phase\": \"PHASE3\"}}<br></span><span class=\"stderr\">2020-04-23 07:43:46,488 INFO: [get_encryption_arguments] driver_host = devbig323.ftw3.facebook.com, driver_datacenter = ftw3, driver_region = ftw, executors_datacenter = atn3, executors_region = atn, cluster = dw-atn-spark1<br></span><span class=\"stderr\">2020-04-23 07:43:46,488 INFO: xregion traffic detected. driver_region = ftw and executors_region = atn<br></span><span class=\"stderr\">2020-04-23 07:43:46,488 DEBUG: Encryption arguments {'spark.ssl.rpc.enabled': 'true', 'spark.ssl.shuffle.enabled': 'true', 'spark.ssl.blocktransfer.enabled': 'true', 'spark.ssl.other.enabled': 'true', 'spark.fb.only.files.distributeViaGFS': 'true', 'spark.bumblebee.tags': 'xregion_wrapper_enabled_encryption'}<br></span><span class=\"stderr\">2020-04-23 07:43:46,488 INFO: Override spark configs: None<br></span><span class=\"stderr\">2020-04-23 07:43:48,276 DEBUG: spark_jars :/usr/local/fbprojects/packages/hive.udfs/652/core-udfs-1.0-SNAPSHOT-standalone.jar:spark-internal<br></span><span class=\"stderr\">2020-04-23 07:43:48,276 INFO: Logging to /tmp/jsc/spark.2020_4_23_7_43.166054.log<br></span><span class=\"stderr\">2020-04-23 07:43:48,276 DEBUG: QUEUE=dw-atn-spark1_adhoc<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: CLUSTER=dw-atn-spark1<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: UTILITY=spark-submit<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: TEMPFS=tempfs://ws.dw.atn3oxygen/spark<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: WS_USER_ENV=atnspark1_atn3<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 INFO: namespace instagram uses spark version 1544<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: ------------ ENVIRONMENT VARIABLES -----------<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: SPARK_USER=jsc<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: SPARK_NAMESPACE=instagram<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: SPARK_UTILITY=dataswarm<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: ORIGINAL_SPARK_RELEASE=1544<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: SPARK_PHASE=phase3<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: SPARK_CONF_DIR=/var/facebook/configerator-client/standalone_configs/datainfra/dw-atn-spark1/spark/common<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: SPARK_HOME_DIR=/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: SPARK_HOME=/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: SPARK_RELEASE=1544<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: SPARK_DATACENTER=atn3<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: SPARK_QUEUE=dw-atn-spark1_adhoc<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: SPARK_TEMPFS=tempfs://ws.dw.atn3oxygen/spark<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: SPARK_WRAPPER_VERSION=135<br></span><span class=\"stderr\">2020-04-23 07:43:48,277 DEBUG: SPARK_APP_METRICS_PATH=/tmp/tmpcztohdmf/spark_app_metrics.json<br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: SPARK_JOB_PRIORITY=NORMAL<br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: SPARK_ERROR_CLASSIFICAITON_OUTPUT_FILE=/tmp/tmpqa0fb0gq/classification<br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: SPARK_BUMBLEBEE_DIST_PACKAGES=fbpkg:///fblearner.plasma:1544#fblearner.plasma.tar.gz<br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: SPARK_CONFIGERATOR_API_ENABLED=true<br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: SPARK_ENV_LOADED=1<br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: SPARK_SUBMIT_OPTS= -noverify -Dscala.usejavacp=true -Dspark.driver.host=devbig323.ftw3.facebook.com -XX:+UseG1GC -XX:+PerfDisableSharedMem -XX:+PreserveFramePointer -XX:+ExplicitGCInvokesConcurrent -XX:+UseGCOverheadLimit -XX:+PrintGCCause -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -XX:+PrintClassHistogramAfterFullGC -XX:+PrintClassHistogramBeforeFullGC -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCApplicationStoppedTime -XX:+PrintHeapAtGC -Xloggc:/tmp/sparkgc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=1G -XX:ErrorFile=/tmp/hs_err_pid%p.log -Dfb.spark.log.file=/tmp/jsc/spark.2020_4_23_7_43.166054.log -Dfb.spark.console.logger.level=INFO <br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: BENTO_KERNEL_EXTENSION=TRUE<br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: BUMBLEBEE_CLUSTER=dw-atn-spark1<br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: CHECK_PATH=/TIER/bumblebee/not-specified<br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: CLICOLOR=1<br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: DS_FLOW_USER_OVERRIDE=jsc<br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: FB_HIVE_UDF_INSTRUMENTATION_ENABLED=false<br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: FB_HIVE_UDF_LOGGING_ENABLED=false<br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: FB_PAR_RUNTIME_FILES=/mnt/xarfuse/uid-117004/e48c97ee-ns-4026531840<br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: FB_PYTHON_COMMAND=/usr/local/fbcode/platform007/bin/python3.7 -tt<br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: FB_XAR_INVOKED_NAME=/usr/local/bin/spark-wrapper<br></span><span class=\"stderr\">2020-04-23 07:43:48,278 DEBUG: FB_XAR_MAIN_MODULE=datainfra.spark.spark_wrapper.launcher<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: GIT_PAGER=cat<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: HOME=/home/jsc<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: INVOCATION_ID=a1e3dcd9149942b090bdf71ade787c26<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: IPYTHONDIR=/data/users/jsc/.bento/ipython<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: JAVA_HOME=/usr/local/jdk-8u60-64/<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: JOURNAL_STREAM=9:1937356474<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: JPY_PARENT_PID=2979575<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: JUPYTER_CONFIG_DIR=/data/users/jsc/.bento/jupyter<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: KRB5CCNAME=FILE:/var/run/ccache/krb5cc_117004<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: LANG=en_US.UTF-8<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: LC_ALL=<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: LC_CTYPE=en_US.UTF-8<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: LD_LIBRARY_PATH=/usr/local/spark/lib/ext:<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: LOGNAME=jsc<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: MATPLOTLIBDATA=/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/matplotlib/mpl-data<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: MATPLOTLIBRC=/data/users/jsc/.bento/matplotlib/matplotlibrc<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: MESOS_NATIVE_JAVA_LIBRARY=/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/lib/ext/libmesos.so<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: MPLBACKEND=module://ipykernel.pylab.backend_inline<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: OVERRIDDEN_APPLICATION_CONFIGS={\"spark.driver.maxResultSize\": \"8g\", \"spark.hive.outerjoin.supports.filters\": \"false\", \"spark.sql.autoShuffledHashJoinThreshold\": \"67108864\", \"spark.sql.dynamicJoin\": \"true\", \"spark.sql.enable.autopartition\": \"true\", \"spark.sql.estimate.stats\": \"true\", \"spark.sql.join.preferSortMergeJoin\": \"false\", \"spark.sql.shuffle.partition.multiplier\": \"0.125\", \"spark.sql.shuffle.partitions.max\": \"10000\", \"spark.sql.shuffle.partitions.min\": \"997\", \"spark.sql.switch.join\": \"true\"}<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: PAGER=cat<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: PWD=/data/users/jsc/personal/covid<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: PYTHONPATH=/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/python/plasma/:/mnt/xarfuse/uid-117004/7fcde8e6-ns-4026531840<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: RUNTIME_DIRECTORY=/run/bento.jsc<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: SCRIBE_LOG_USAGE=yes<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: SHELL=/bin/bash<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: SHLVL=1<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: TERM=xterm-color<br></span><span class=\"stderr\">2020-04-23 07:43:48,279 DEBUG: THRIFT_TLS_CL_CERT_PATH=/var/facebook/credentials/jsc/x509/jsc.pem<br></span><span class=\"stderr\">2020-04-23 07:43:48,300 DEBUG: THRIFT_TLS_CL_KEY_PATH=/var/facebook/credentials/jsc/x509/jsc.pem<br></span><span class=\"stderr\">2020-04-23 07:43:48,300 DEBUG: TTFPATH=/mnt/xarfuse/uid-117004/8fc84e00-ns-4026531840/matplotlib/mpl-data/fonts/ttf<br></span><span class=\"stderr\">2020-04-23 07:43:48,300 DEBUG: UDFS_PHASE=phase3<br></span><span class=\"stderr\">2020-04-23 07:43:48,300 DEBUG: UDFS_VERSION=652<br></span><span class=\"stderr\">2020-04-23 07:43:48,300 DEBUG: USER=jsc<br></span><span class=\"stderr\">2020-04-23 07:43:48,300 DEBUG: XARFUSE_NEW_MOUNT=1<br></span><span class=\"stderr\">2020-04-23 07:43:48,300 DEBUG: XDG_RUNTIME_DIR=/run/bento.jsc<br></span><span class=\"stderr\">2020-04-23 07:43:48,301 DEBUG: ------------<br></span><span class=\"stderr\">2020-04-23 07:43:48,301 DEBUG: CONFIGURATION={\"spark.bumblebee.local.mode.dataswarm.operators\": \"DynaswarmHive2MySQLOperator,HDFS2HiveOperator,Hive2CompoundEyeOperator,Hive2HDFSOperator,HiveTableDifferenceOperator2,MySQL2HiveOperator,ODS2HiveOperator,PrecompPublishPartition2MySQLOperator,Rapido2HiveOperator,Salesforce2HiveOperator,Scuba2HiveOperator,ScubaJoin2HiveOperator,SQLServer2HiveOperator,HivePublishSignalTableOperator,ROperator,LlamasSkewHiveOp,HivePartitionSizeCheckOperator,TableGroupHiveQLOperator,HiveRowCountCheckOperator,DynamicPipelineOperator,HiveQLCheckDifferenceOperator\", \"spark.bumblebee.local.mode.override.enabled\": \"true\", \"spark.config.api.enabled\": \"true\", \"spark.driver.jstack.logging\": \"false\", \"spark.fb.only.jars.distributeViaBumblebee\": \"true\", \"spark.fb.only.nssr4.enabled\": \"true\", \"wrapper.atnspark_use_dc_level_queues\": \"false\", \"wrapper.change.current.nonwritable.dir\": \"false\", \"wrapper.java_home\": \"/usr/local/fb-jdk-8.102\", \"wrapper.log.spark_app_metrics\": \"true\", \"wrapper.log.xdb.spark_app_metrics\": \"true\", \"wrapper.plasma_add_udfs\": \"true\", \"wrapper.plasma_udf_version\": \"1360\", \"spark.driver.maxResultSize\": \"8g\", \"spark.hive.outerjoin.supports.filters\": \"false\", \"spark.sql.autoShuffledHashJoinThreshold\": \"67108864\", \"spark.sql.dynamicJoin\": \"true\", \"spark.sql.enable.autopartition\": \"true\", \"spark.sql.estimate.stats\": \"true\", \"spark.sql.join.preferSortMergeJoin\": \"false\", \"spark.sql.shuffle.partition.multiplier\": \"0.125\", \"spark.sql.shuffle.partitions.max\": \"10000\", \"spark.sql.shuffle.partitions.min\": \"997\", \"spark.sql.switch.join\": \"true\", \"spark.app.name\": \"pvc.spark.instagram\", \"spark.udfs.version\": \"phase3\", \"spark.cosco.enable.reducer.task.upper.count\": \"6000\", \"spark.fb.only.label.based.scheduling.enabled\": \"true\", \"spark.fb.only.tetris.enabled.status\": \"logonly\", \"spark.executor.extraLibraryPath\": \"fblearner.plasma.tar.gz/lib/ext:/packages/spark.mkl:/usr/local/java-runtime/impl/8/jre/lib/amd64/server\", \"spark.history.based.tuning.shuffle.partitions.maxShufflePartitions\": \"6000\", \"spark.history.based.tuning.shuffle.partitions.minShufflePartitions\": \"200\", \"spark.history.based.tuning.shuffle.partitions.override.enabled\": \"true\", \"spark.history.based.tuning.shuffle.partitions.override.fraction\": \"1.0\", \"spark.history.based.tuning.shuffle.targetPostShuffleInputSize\": \"900000000\", \"wrapper_scuba_sample\": {\"int\": {\"time\": 1587653026, \"cpu_cores\": 80, \"total_memory_kb\": \"263652028\", \"run_time\": 4}, \"normal\": {\"user\": \"jsc\", \"host\": \"devbig323.ftw3.facebook.com\", \"queue\": \"dw-atn-spark1_adhoc\", \"cluster\": \"dw-atn-spark1\", \"namespace\": \"instagram\", \"datacenter\": \"atn3\", \"tempfs\": \"tempfs://ws.dw.atn3oxygen/spark\", \"SPARK_RELEASE\": \"1544\", \"SPARK_PHASE\": \"phase3\", \"cmd_line\": \"'/usr/local/bin/spark-wrapper' '--debug' 'run-utility' 'flow-submit' '-j' '/usr/local/jdk-8u60-64/' '-q' 'instagram/adhoc' '-u' 'jsc' '-e' 'spark.driver.maxResultSize=8g' '-e' 'spark.hive.outerjoin.supports.filters=false' '-e' 'spark.sql.autoShuffledHashJoinThreshold=67108864' '-e' 'spark.sql.dynamicJoin=true' '-e' 'spark.sql.enable.autopartition=true' '-e' 'spark.sql.estimate.stats=true' '-e' 'spark.sql.join.preferSortMergeJoin=false' '-e' 'spark.sql.shuffle.partition.multiplier=0.125' '-e' 'spark.sql.shuffle.partitions.max=10000' '-e' 'spark.sql.shuffle.partitions.min=997' '-e' 'spark.sql.switch.join=true' '-C' 'org.apache.spark.generichql.GenericHQL' '{\\\"appname\\\": \\\"pvc.spark.instagram\\\", \\\"namespace\\\": \\\"instagram\\\", \\\"queries\\\": [{\\\"show\\\": false, \\\"splitQueries\\\": true, \\\"cache\\\": false, \\\"repartition\\\": 0, \\\"coalesce\\\": 0, \\\"assertCheck\\\": false, \\\"alias\\\": \\\"\\\", \\\"hql\\\": \\\"\\\\n    \\\\n    CREATE TABLE IF NOT EXISTS tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d (\\\\n        fips INT, label FLOAT\\\\n    )\\\\n    ROW FORMAT DELIMITED\\\\n    FIELDS TERMINATED BY '\\\\\\\\001'\\\\n    COLLECTION ITEMS TERMINATED BY '\\\\\\\\002'\\\\n    MAP KEYS TERMINATED BY '\\\\\\\\003'\\\\n    LINES TERMINATED BY '\\\\\\\\n'\\\\n    STORED AS TEXTFILE\\\\n    TBLPROPERTIES ('RETENTION' = '7');\\\\n\\\\n    LOAD DATA LOCAL INPATH '/mnt/vol/gfsfblearner-texas2/flow/data/2020-04-23/d0868775-82e9-48d4-99d8-1509d8bf068b'\\\\n    OVERWRITE INTO TABLE tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;\\\\n\\\\n    \\\\n    CREATE TABLE IF NOT EXISTS tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8 (\\\\n        fips INT, label FLOAT\\\\n    )\\\\n    PARTITIONED BY (ds STRING, number STRING)\\\\n    \\\\n    TBLPROPERTIES('RETENTION' = '180');\\\\n    \\\\n    \\\\n    CREATE DEPENDENT TABLE IF NOT EXISTS tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8_signal\\\\n    REFERS TO TABLE tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8\\\\n    GROUPED BY (ds);\\\\n    \\\\n    \\\\n    INSERT OVERWRITE TABLE tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8\\\\n        PARTITION (ds = \\\\\\\"2020-04-21\\\\\\\", number = \\\\\\\"1\\\\\\\")\\\\n        SELECT fips, label FROM tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;;\\\\n    \\\\n\\\\n    DROP TABLE IF EXISTS tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;\\\\n    \\\"}]}'\", \"spark_utility\": \"dataswarm\", \"r_release\": \"3.6.1\", \"spark_wrapper_version\": \"135\", \"kernel\": \"4.16.18-196_fbk21_5305_g20f611be3d50\", \"os_release\": \"CentOS Linux release 7.6.1810 (Core) \", \"python_path\": \"/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/python/plasma/:/mnt/xarfuse/uid-117004/7fcde8e6-ns-4026531840\", \"working_dir\": \"/data/users/jsc/personal/covid\", \"chronos_job_instance_id\": null, \"chronos_job_name\": null, \"operation\": \"START\", \"invalid_fbpkg\": \"null\", \"classification\": null, \"exit_code\": \"0\", \"wrapper_exception\": null, \"hive_query_source_json\": null, \"original_spark_release\": \"1544\", \"udfs_version\": \"652\", \"udfs_phase\": \"phase3\", \"previous_runs_failure_info\": \"{}\", \"overridden_application_configs\": \"{\\\"spark.driver.maxResultSize\\\": \\\"8g\\\", \\\"spark.hive.outerjoin.supports.filters\\\": \\\"false\\\", \\\"spark.sql.autoShuffledHashJoinThreshold\\\": \\\"67108864\\\", \\\"spark.sql.dynamicJoin\\\": \\\"true\\\", \\\"spark.sql.enable.autopartition\\\": \\\"true\\\", \\\"spark.sql.estimate.stats\\\": \\\"true\\\", \\\"spark.sql.join.preferSortMergeJoin\\\": \\\"false\\\", \\\"spark.sql.shuffle.partition.multiplier\\\": \\\"0.125\\\", \\\"spark.sql.shuffle.partitions.max\\\": \\\"10000\\\", \\\"spark.sql.shuffle.partitions.min\\\": \\\"997\\\", \\\"spark.sql.switch.join\\\": \\\"true\\\"}\", \"configs_phase\": \"PHASE3\"}}, \"spark.ssl.rpc.enabled\": \"true\", \"spark.ssl.shuffle.enabled\": \"true\", \"spark.ssl.blocktransfer.enabled\": \"true\", \"spark.ssl.other.enabled\": \"true\", \"spark.fb.only.files.distributeViaGFS\": \"true\", \"spark.bumblebee.tags\": \"xregion_wrapper_enabled_encryption\", \"spark.bumblebee.dist.packages\": \"fbpkg:///hive.udfs:652#hive.udfs\", \"spark.driver.extraClassPath\": \"/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/lib/ext/*:/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/jars/*:/var/facebook/configerator-client/standalone_configs/datainfra/dw-atn-spark1:/usr/local/fbprojects/packages/hive.udfs/652/core-udfs-1.0-SNAPSHOT-standalone.jar:spark-internal\", \"spark.driver.extraLibraryPath\": \"/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/lib/ext/\", \"spark.bumblebee.queue\": \"dw-atn-spark1_adhoc\", \"spark.remote.io.rootDir\": \"tempfs://ws.dw.atn3oxygen/spark\", \"spark.remote.io.hconf.ws.user.env\": \"atnspark1_atn3\", \"spark.ws.user.env\": \"atnspark1_atn3\"}<br></span><span class=\"stderr\">2020-04-23 07:43:48,341 DEBUG: EXTRA ARGUMENTS=['--driver-class-path', '/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/lib/ext/*:/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/jars/*:/var/facebook/configerator-client/standalone_configs/datainfra/dw-atn-spark1:/usr/local/fbprojects/packages/hive.udfs/652/core-udfs-1.0-SNAPSHOT-standalone.jar:spark-internal', '--driver-library-path', '/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/lib/ext/', '--class', 'org.apache.spark.generichql.GenericHQL', '--verbose', 'spark-internal', '{\"appname\": \"pvc.spark.instagram\", \"namespace\": \"instagram\", \"queries\": [{\"show\": false, \"splitQueries\": true, \"cache\": false, \"repartition\": 0, \"coalesce\": 0, \"assertCheck\": false, \"alias\": \"\", \"hql\": \"\\\\n    \\\\n    CREATE TABLE IF NOT EXISTS tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d (\\\\n        fips INT, label FLOAT\\\\n    )\\\\n    ROW FORMAT DELIMITED\\\\n    FIELDS TERMINATED BY \\'\\\\\\\\001\\'\\\\n    COLLECTION ITEMS TERMINATED BY \\'\\\\\\\\002\\'\\\\n    MAP KEYS TERMINATED BY \\'\\\\\\\\003\\'\\\\n    LINES TERMINATED BY \\'\\\\\\\\n\\'\\\\n    STORED AS TEXTFILE\\\\n    TBLPROPERTIES (\\'RETENTION\\' = \\'7\\');\\\\n\\\\n    LOAD DATA LOCAL INPATH \\'/mnt/vol/gfsfblearner-texas2/flow/data/2020-04-23/d0868775-82e9-48d4-99d8-1509d8bf068b\\'\\\\n    OVERWRITE INTO TABLE tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;\\\\n\\\\n    \\\\n    CREATE TABLE IF NOT EXISTS tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8 (\\\\n        fips INT, label FLOAT\\\\n    )\\\\n    PARTITIONED BY (ds STRING, number STRING)\\\\n    \\\\n    TBLPROPERTIES(\\'RETENTION\\' = \\'180\\');\\\\n    \\\\n    \\\\n    CREATE DEPENDENT TABLE IF NOT EXISTS tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8_signal\\\\n    REFERS TO TABLE tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8\\\\n    GROUPED BY (ds);\\\\n    \\\\n    \\\\n    INSERT OVERWRITE TABLE tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8\\\\n        PARTITION (ds = \\\\\"2020-04-21\\\\\", number = \\\\\"1\\\\\")\\\\n        SELECT fips, label FROM tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;;\\\\n    \\\\n\\\\n    DROP TABLE IF EXISTS tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;\\\\n    \"}]}']<br></span><span class=\"stderr\">2020-04-23 07:43:48,343 DEBUG: Run command: ['/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/bin/spark-submit', '--conf', 'spark.bumblebee.local.mode.dataswarm.operators=DynaswarmHive2MySQLOperator,HDFS2HiveOperator,Hive2CompoundEyeOperator,Hive2HDFSOperator,HiveTableDifferenceOperator2,MySQL2HiveOperator,ODS2HiveOperator,PrecompPublishPartition2MySQLOperator,Rapido2HiveOperator,Salesforce2HiveOperator,Scuba2HiveOperator,ScubaJoin2HiveOperator,SQLServer2HiveOperator,HivePublishSignalTableOperator,ROperator,LlamasSkewHiveOp,HivePartitionSizeCheckOperator,TableGroupHiveQLOperator,HiveRowCountCheckOperator,DynamicPipelineOperator,HiveQLCheckDifferenceOperator', '--conf', 'spark.bumblebee.local.mode.override.enabled=true', '--conf', 'spark.config.api.enabled=true', '--conf', 'spark.driver.jstack.logging=false', '--conf', 'spark.fb.only.jars.distributeViaBumblebee=true', '--conf', 'spark.fb.only.nssr4.enabled=true', '--conf', 'spark.driver.maxResultSize=8g', '--conf', 'spark.hive.outerjoin.supports.filters=false', '--conf', 'spark.sql.autoShuffledHashJoinThreshold=67108864', '--conf', 'spark.sql.dynamicJoin=true', '--conf', 'spark.sql.enable.autopartition=true', '--conf', 'spark.sql.estimate.stats=true', '--conf', 'spark.sql.join.preferSortMergeJoin=false', '--conf', 'spark.sql.shuffle.partition.multiplier=0.125', '--conf', 'spark.sql.shuffle.partitions.max=10000', '--conf', 'spark.sql.shuffle.partitions.min=997', '--conf', 'spark.sql.switch.join=true', '--conf', 'spark.app.name=pvc.spark.instagram', '--conf', 'spark.udfs.version=phase3', '--conf', 'spark.cosco.enable.reducer.task.upper.count=6000', '--conf', 'spark.fb.only.label.based.scheduling.enabled=true', '--conf', 'spark.fb.only.tetris.enabled.status=logonly', '--conf', 'spark.executor.extraLibraryPath=fblearner.plasma.tar.gz/lib/ext:/packages/spark.mkl:/usr/local/java-runtime/impl/8/jre/lib/amd64/server', '--conf', 'spark.history.based.tuning.shuffle.partitions.maxShufflePartitions=6000', '--conf', 'spark.history.based.tuning.shuffle.partitions.minShufflePartitions=200', '--conf', 'spark.history.based.tuning.shuffle.partitions.override.enabled=true', '--conf', 'spark.history.based.tuning.shuffle.partitions.override.fraction=1.0', '--conf', 'spark.history.based.tuning.shuffle.targetPostShuffleInputSize=900000000', '--conf', 'spark.ssl.rpc.enabled=true', '--conf', 'spark.ssl.shuffle.enabled=true', '--conf', 'spark.ssl.blocktransfer.enabled=true', '--conf', 'spark.ssl.other.enabled=true', '--conf', 'spark.fb.only.files.distributeViaGFS=true', '--conf', 'spark.bumblebee.tags=xregion_wrapper_enabled_encryption', '--conf', 'spark.bumblebee.dist.packages=fbpkg:///hive.udfs:652#hive.udfs', '--conf', 'spark.driver.extraClassPath=/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/lib/ext/*:/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/jars/*:/var/facebook/configerator-client/standalone_configs/datainfra/dw-atn-spark1:/usr/local/fbprojects/packages/hive.udfs/652/core-udfs-1.0-SNAPSHOT-standalone.jar:spark-internal', '--conf', 'spark.driver.extraLibraryPath=/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/lib/ext/', '--conf', 'spark.bumblebee.queue=dw-atn-spark1_adhoc', '--conf', 'spark.remote.io.rootDir=tempfs://ws.dw.atn3oxygen/spark', '--conf', 'spark.remote.io.hconf.ws.user.env=atnspark1_atn3', '--conf', 'spark.ws.user.env=atnspark1_atn3', '--driver-class-path', '/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/lib/ext/*:/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/jars/*:/var/facebook/configerator-client/standalone_configs/datainfra/dw-atn-spark1:/usr/local/fbprojects/packages/hive.udfs/652/core-udfs-1.0-SNAPSHOT-standalone.jar:spark-internal', '--driver-library-path', '/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/lib/ext/', '--class', 'org.apache.spark.generichql.GenericHQL', '--verbose', 'spark-internal', '{\"appname\": \"pvc.spark.instagram\", \"namespace\": \"instagram\", \"queries\": [{\"show\": false, \"splitQueries\": true, \"cache\": false, \"repartition\": 0, \"coalesce\": 0, \"assertCheck\": false, \"alias\": \"\", \"hql\": \"\\\\n    \\\\n    CREATE TABLE IF NOT EXISTS tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d (\\\\n        fips INT, label FLOAT\\\\n    )\\\\n    ROW FORMAT DELIMITED\\\\n    FIELDS TERMINATED BY \\'\\\\\\\\001\\'\\\\n    COLLECTION ITEMS TERMINATED BY \\'\\\\\\\\002\\'\\\\n    MAP KEYS TERMINATED BY \\'\\\\\\\\003\\'\\\\n    LINES TERMINATED BY \\'\\\\\\\\n\\'\\\\n    STORED AS TEXTFILE\\\\n    TBLPROPERTIES (\\'RETENTION\\' = \\'7\\');\\\\n\\\\n    LOAD DATA LOCAL INPATH \\'/mnt/vol/gfsfblearner-texas2/flow/data/2020-04-23/d0868775-82e9-48d4-99d8-1509d8bf068b\\'\\\\n    OVERWRITE INTO TABLE tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;\\\\n\\\\n    \\\\n    CREATE TABLE IF NOT EXISTS tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8 (\\\\n        fips INT, label FLOAT\\\\n    )\\\\n    PARTITIONED BY (ds STRING, number STRING)\\\\n    \\\\n    TBLPROPERTIES(\\'RETENTION\\' = \\'180\\');\\\\n    \\\\n    \\\\n    CREATE DEPENDENT TABLE IF NOT EXISTS tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8_signal\\\\n    REFERS TO TABLE tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8\\\\n    GROUPED BY (ds);\\\\n    \\\\n    \\\\n    INSERT OVERWRITE TABLE tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8\\\\n        PARTITION (ds = \\\\\"2020-04-21\\\\\", number = \\\\\"1\\\\\")\\\\n        SELECT fips, label FROM tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;;\\\\n    \\\\n\\\\n    DROP TABLE IF EXISTS tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;\\\\n    \"}]}']<br></span><span class=\"stderr\">2020-04-23 07:43:48,350 DEBUG: Timeout is 604800<br></span><span class=\"stderr\">20/04/23 07:43:48 INFO Utils: File relative path: datainfra/dw-atn-spark1/spark/common/spark-defaults.conf<br></span><span class=\"stderr\">Using properties file: datainfra/dw-atn-spark1/spark/common/spark-defaults.conf<br></span><span class=\"stderr\">20/04/23 07:43:48 INFO Utils: Getting datainfra/dw-atn-spark1/spark/common/spark-defaults.conf from configerator API<br></span><span class=\"stderr\">20/04/23 07:43:49 INFO Proxy2Connection: New proxy2 session id: devbig323.ftw3.facebook.com:1978986:1586492492981690:2390910<br></span><span class=\"stderr\">Adding default property: spark.remote.io.shuffle.directFetch=false<br></span><span class=\"stderr\">Adding default property: spark.fb.only.mimic.fbhive.outer.join=true<br></span><span class=\"stderr\">Adding default property: spark.shuffle.merge.merger.optionalMergeThreshold=0.95<br></span><span class=\"stderr\">Adding default property: spark.fb.only.externalUnsafeRowLinkedListLogStats=true<br></span><span class=\"stderr\">Adding default property: spark.hadoop.dfs.fastcopy.thread.pool.size=64<br></span><span class=\"stderr\">Adding default property: spark.remote.io.hconf.ws.dedicated.client.proxy.tier.suffix=tt_twdi<br></span><span class=\"stderr\">Adding default property: spark.port.maxRetries=128<br></span><span class=\"stderr\">Adding default property: spark.config.memory.override.numDaysToLookBack=7<br></span><span class=\"stderr\">Adding default property: spark.cosco.task.use.package.id.optimized=true<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.partition.lease.second=86400<br></span><span class=\"stderr\">Adding default property: spark.remote.io.retry.errorCodes=5<br></span><span class=\"stderr\">Adding default property: spark.serializer=org.apache.spark.serializer.KryoSerializer<br></span><span class=\"stderr\">Adding default property: spark.rpc.io.backLog=8192<br></span><span class=\"stderr\">Adding default property: spark.cosco.writer.netty.package.transfer.timeout.ms=55000<br></span><span class=\"stderr\">Adding default property: spark.remote.io.hconf.tempfs.inputstream.buffer.bytes.OVERRIDE=4194304<br></span><span class=\"stderr\">Adding default property: spark.blacklist.enabled=false<br></span><span class=\"stderr\">Adding default property: spark.executorEnv.FB_HIVE_UDF_LOGGING_ENABLED=false<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.check.permission.output.table=true<br></span><span class=\"stderr\">Adding default property: spark.speculation=true<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.dynamicAllocation.watcher.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.ssl.blocktransfer.enabled=false<br></span><span class=\"stderr\">Adding default property: spark.config.broadwell1.onHeap.size.override=4g<br></span><span class=\"stderr\">Adding default property: spark.hlt.new.logger.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.io.compression.lz4.blockSize=512k<br></span><span class=\"stderr\">Adding default property: spark.cosco.task.reducer.0.retry.regex=.*(loadshedding request|WSE_BLOCK_READ_NOT_INITIATED|WSE_BLOCK_CHUNK_MISSING|WSE_THRIFT_GENERAL_ERROR|WSE_ZDB_RETRYABLE_ERROR|WSE_THROTTLED|InterruptedIOException|WSE_TASK_EXPIRED|Could not acquire mod lock|Could not acquire fetch lock|java.io.IOException: java.lang.InterruptedException).*<br></span><span class=\"stderr\">Adding default property: spark.executor.extraJavaOptions=-noverify -Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses=true -Dscala.usejavacp=true -Djava.security.egd=file:///dev/urandom -XX:+PreserveFramePointer -XX:+UnlockDiagnosticVMOptions -XX:+DebugNonSafepoints -XX:FreqInlineSize=128 -XX:+UnlockExperimentalVMOptions -XX:ParallelGCThreads=4 -XX:+UseParallelGC -XX:+UseParallelOldGC -XX:+PrintAdaptiveSizePolicy -XX:+PerfDisableSharedMem -XX:+PrintGCCause -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -XX:+PrintClassHistogramAfterFullGC -XX:+PrintClassHistogramBeforeFullGC -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCApplicationStoppedTime -XX:+PrintHeapAtGC -Xloggc:/tmp/spark.executor.gc.%p.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=20M -XX:ErrorFile=/tmp/hs_err_pid.%p.log -XX:OnOutOfMemoryError=\"echo THIS_IS_JVM_OOM &gt;&amp;2 &amp;&amp; (/bin/kill -9 %p &amp;&amp; echo KILLED &gt;&amp;2) &amp;\" -XX:-OmitStackTraceInFastThrow<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.failOnSortingValidation=true<br></span><span class=\"stderr\">Adding default property: spark.shuffle.merge.merger.blockSizeThreshold=4m<br></span><span class=\"stderr\">Adding default property: spark.history.fs.logDirectory=ws://ws.dw.atn3oxygen/user/hadoop/dw-atn-spark1/spark/logs<br></span><span class=\"stderr\">Adding default property: spark.fb.only.config.validator.fail.on.violation=true<br></span><span class=\"stderr\">Adding default property: spark.ws.user.name=spark<br></span><span class=\"stderr\">Adding default property: spark.sql.autoBroadcastJoinThreshold=75000000<br></span><span class=\"stderr\">Adding default property: spark.io.compression.codec=zstd<br></span><span class=\"stderr\">Adding default property: spark.fb.only.tetris.enabled.status=disabled<br></span><span class=\"stderr\">Adding default property: spark.eventLog.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.executor.heartbeatInterval=60s<br></span><span class=\"stderr\">Adding default property: spark.config.memory.override.fraction=1.0<br></span><span class=\"stderr\">Adding default property: spark.config.skylake.memory.override.enable=true<br></span><span class=\"stderr\">Adding default property: spark.config.broadwell1.memory.override.enable=true<br></span><span class=\"stderr\">Adding default property: spark.shuffle.manager=org.apache.spark.shuffle.cosco.CoscoShuffleManager<br></span><span class=\"stderr\">Adding default property: spark.fb.only.default.executor.offHeap.memory=4g<br></span><span class=\"stderr\">Adding default property: spark.ws.dedicated.client.proxy.tier.suffix=tt_twdi<br></span><span class=\"stderr\">Adding default property: spark.sql.adaptive.skewJoin.enabled=false<br></span><span class=\"stderr\">Adding default property: spark.config.skylake.offHeap.size.override=6g<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.executor.resource.tuning.enabled.status=enabled<br></span><span class=\"stderr\">Adding default property: spark.hadoop.mapred.autosplit.input.threshold=10995116277760<br></span><span class=\"stderr\">Adding default property: spark.config.broadwell1.offHeap.size=4g<br></span><span class=\"stderr\">Adding default property: spark.driver.maxResultSize=6g<br></span><span class=\"stderr\">Adding default property: spark.sql.shuffle.partitions.max=6000<br></span><span class=\"stderr\">Adding default property: spark.remote.io.class=org.apache.hadoop.fs.TempFileSystemShim<br></span><span class=\"stderr\">Adding default property: spark.remote.io.hconf.tempfs.use.streaming.read.OVERRIDE=false<br></span><span class=\"stderr\">Adding default property: spark.skew.detection.minSize=10737418240<br></span><span class=\"stderr\">Adding default property: spark.fb.only.memory.offHeap.size.limit=9g<br></span><span class=\"stderr\">Adding default property: spark.fb.only.maxRowLimit=100000<br></span><span class=\"stderr\">Adding default property: spark.config.skylake.onHeap.size.override=4g<br></span><span class=\"stderr\">Adding default property: spark.history.based.tuning.offHeapSize.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.ui.port=8087<br></span><span class=\"stderr\">Adding default property: spark.shuffle.sort.bypassMergeThreshold=10<br></span><span class=\"stderr\">Adding default property: spark.executorEnv.BUMBLEBEE_CLUSTER=dw-atn-spark1<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.native.udfs.use.custom=if,collect_set<br></span><span class=\"stderr\">Adding default property: spark.shuffle.service.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.native.udfs.suppressed=variance,to_date,fb_array_unique,fb_sum_where,fb_top,max_by,fb_choose_one,fb_collect_map<br></span><span class=\"stderr\">Adding default property: spark.config.memory.override.blacklistKds=true<br></span><span class=\"stderr\">Adding default property: spark.extraListeners=org.apache.spark.scheduler.SparkScubaLogger,org.apache.spark.sql.StagingLocationCleaner<br></span><span class=\"stderr\">Adding default property: spark.eventLog.buffer.kb=1024<br></span><span class=\"stderr\">Adding default property: spark.cosco.uow.codec=com.facebook.di.cosco.AircompressorZstdCodec<br></span><span class=\"stderr\">Adding default property: spark.driver.extraLibraryPath=/usr/local/spark/lib/ext/<br></span><span class=\"stderr\">Adding default property: spark.hadoop.mapred.max.num.blocks.per.split=2000<br></span><span class=\"stderr\">Adding default property: spark.cosco.netty.connection.retry.number=64<br></span><span class=\"stderr\">Adding default property: spark.ws.thrift.chunking.version.OVERRIDE=4<br></span><span class=\"stderr\">Adding default property: spark.shuffle.service.registration.timeout=180000<br></span><span class=\"stderr\">Adding default property: spark.fb.only.executor.memory.limit=24g<br></span><span class=\"stderr\">Adding default property: spark.mapred.fs.friendly.bucket.split=true<br></span><span class=\"stderr\">Adding default property: spark.fb.only.partial.agg.override.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.ssl.pemFilePath=/var/facebook/x509_identities/server.pem<br></span><span class=\"stderr\">Adding default property: spark.config.broadwell2.onHeap.size=8g<br></span><span class=\"stderr\">Adding default property: spark.hadoop.hive.orc.hll.collection.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.fb.only.heterogeneous.hardware.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.sql.parser.quotedRegexColumnNames=true<br></span><span class=\"stderr\">Adding default property: spark.io.compression.zstd.bufferSize=512k<br></span><span class=\"stderr\">Adding default property: spark.io.compression.snappy.blockSize=1m<br></span><span class=\"stderr\">Adding default property: spark.skew.detection.median.ratio=10<br></span><span class=\"stderr\">Adding default property: spark.config.skylake.offHeap.size=4g<br></span><span class=\"stderr\">Adding default property: spark.remote.io.hconf.ws.use.streaming.read.OVERRIDE=true<br></span><span class=\"stderr\">Adding default property: spark.fb.only.task.no.progress.kill.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.isParallelWritersForDynamicPart=false<br></span><span class=\"stderr\">Adding default property: spark.files.fetchCachePath=/var/spark<br></span><span class=\"stderr\">Adding default property: spark.fb.only.perOperatorMemoryLimitEnabled=true<br></span><span class=\"stderr\">Adding default property: spark.executor.memory.override=4g<br></span><span class=\"stderr\">Adding default property: spark.shuffle.unsafe.file.output.buffer=32m<br></span><span class=\"stderr\">Adding default property: spark.kryoserializer.buffer.max=1g<br></span><span class=\"stderr\">Adding default property: spark.remote.io.hconf.ws.user.name=spark<br></span><span class=\"stderr\">Adding default property: spark.broadcast.factory=org.apache.spark.broadcast.HttpBroadcastFactory<br></span><span class=\"stderr\">Adding default property: spark.fb.only.task.no.progress.timeout=360m<br></span><span class=\"stderr\">Adding default property: spark.cosco.admission.per.shuffle.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.config.machine.types=skylake,broadwell1,broadwell2<br></span><span class=\"stderr\">Adding default property: spark.ssl.trustStore=/etc/pki/java/fb_certs.jks<br></span><span class=\"stderr\">Adding default property: spark.fb.only.externalAppendOnlyUnsafeRowArrayLogStats=false<br></span><span class=\"stderr\">Adding default property: spark.hlt.logger.enabled=false<br></span><span class=\"stderr\">Adding default property: spark.remote.io.hconf.dw.security.project=spark.storage.security<br></span><span class=\"stderr\">Adding default property: spark.fb.only.useColumnStats=false<br></span><span class=\"stderr\">Adding default property: spark.config.broadwell1.offHeap.size.override=9g<br></span><span class=\"stderr\">Adding default property: spark.dynamicAllocation.cachedExecutorIdleTimeout=10m<br></span><span class=\"stderr\">Adding default property: spark.reducer.shuffle.numAlternateAddresses=5<br></span><span class=\"stderr\">Adding default property: spark.executorEnv.LANG=en_US.UTF-8<br></span><span class=\"stderr\">Adding default property: spark.file.transferTo=false<br></span><span class=\"stderr\">Adding default property: spark.fb.only.dc.based.executor.resource.configs=domains/datainfra/spark/atn_spark1_dc_based_resource_configs<br></span><span class=\"stderr\">Adding default property: spark.sql.orc.vectorized.reader.v2=true<br></span><span class=\"stderr\">Adding default property: spark.tetris.router.tier=tetris.router.prod.spark<br></span><span class=\"stderr\">Adding default property: spark.cosco.task.reducer.1.retry.regex=.*(loadshedding request|WSE_BLOCK_READ_NOT_INITIATED|WSE_BLOCK_CHUNK_MISSING|WSE_THRIFT_GENERAL_ERROR|WSE_THROTTLED|InterruptedIOException|WSE_TASK_EXPIRED|Could not acquire mod lock|Could not acquire fetch lock|java.io.IOException: java.lang.InterruptedException).*<br></span><span class=\"stderr\">Adding default property: spark.ext.h2o.client.web.port=8089<br></span><span class=\"stderr\">Adding default property: spark.fb.only.constant.io.skip.extra.udfs=fb_map_key_apply,fb_accumulate_by_key<br></span><span class=\"stderr\">Adding default property: spark.network.timeout=900s<br></span><span class=\"stderr\">Adding default property: spark.history.based.tuning.appNamesToIgnore=spark-sql,hql_validation_spark,Spark Shell<br></span><span class=\"stderr\">Adding default property: spark.ws.throttle.response.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.memory.offHeap.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.cosco.admission.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.shuffle.io.maxRetries=15<br></span><span class=\"stderr\">Adding default property: spark.speculation.interval=2m<br></span><span class=\"stderr\">Adding default property: spark.remote.io.hconf.ws.chunking.thrift.rollout.pct.tempfs.OVERRIDE=100<br></span><span class=\"stderr\">Adding default property: spark.fb.only.label.based.scheduling.force.dc=platform:atn3,instagram:atn3,groups:atn3<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.universalCombineInputFormat=true<br></span><span class=\"stderr\">Adding default property: spark.fb.only.transform.memory.limit=13g<br></span><span class=\"stderr\">Adding default property: spark.hbt.use.xdb.for.iskds=true<br></span><span class=\"stderr\">Adding default property: spark.fb.only.bypass.nondeterministic.check=true<br></span><span class=\"stderr\">Adding default property: spark.remote.io.hconf.tempfs.thompson.max.async.puts.OVERRIDE=10<br></span><span class=\"stderr\">Adding default property: spark.fb.only.transformEnv.MKL_NUM_THREADS=1<br></span><span class=\"stderr\">Adding default property: spark.driver.memory=5g<br></span><span class=\"stderr\">Adding default property: spark.config.broadwell2.onHeap.size.override=4g<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.ensureBucketing=true<br></span><span class=\"stderr\">Adding default property: spark.shuffle.merge.merger.fileSizeThreshold=5g<br></span><span class=\"stderr\">Adding default property: spark.kill.tasks.on.fetch.failure=true<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.executor.resource.tuning.cpu.maximum.fraction=0.825<br></span><span class=\"stderr\">Adding default property: spark.executorEnv.JAVA_HOME_UNIVERSAL_CLIENT=/usr/local/fb-jdk-8.102<br></span><span class=\"stderr\">Adding default property: spark.rpc.io.serverThreads=64<br></span><span class=\"stderr\">Adding default property: spark.files.fetchFailure.unRegisterOutputOnHost=true<br></span><span class=\"stderr\">Adding default property: spark.shuffle.io.retryWait=20s<br></span><span class=\"stderr\">Adding default property: spark.sql.hive.metastorePartitionPruning=true<br></span><span class=\"stderr\">Adding default property: spark.shuffle.cosco.sorted.shuffle.max.num.shuffles=4<br></span><span class=\"stderr\">Adding default property: spark.ui.showConsoleProgress=true<br></span><span class=\"stderr\">Adding default property: spark.fb.only.aliasAgnosticProjectPartitioning=true<br></span><span class=\"stderr\">Adding default property: spark.remote.io.hconf.tempfs.block.size.OVERRIDE=25165824<br></span><span class=\"stderr\">Adding default property: spark.fb.only.sql.createFileForEmptyBucket=true<br></span><span class=\"stderr\">Adding default property: spark.shuffle.merge.merger=org.apache.spark.scheduler.FixedBlockSizeMerger<br></span><span class=\"stderr\">Adding default property: spark.memory.offHeap.size=4g<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.removeSortAfterJoin=false<br></span><span class=\"stderr\">Adding default property: spark.ssl.shuffle.enabled=false<br></span><span class=\"stderr\">Adding default property: spark.fb.only.final.shuffle.partition.override.enabled=false<br></span><span class=\"stderr\">Adding default property: spark.unsafe.sorter.spill.reader.buffer.size=2m<br></span><span class=\"stderr\">Adding default property: spark.cosco.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.jstack.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.codegen.hive.table.scan.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.config.broadwell2.offHeap.size.override=9g<br></span><span class=\"stderr\">Adding default property: spark.ssl.rpc.enabled=false<br></span><span class=\"stderr\">Adding default property: spark.shuffle.io.clientThreads=4<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.generateJoinRowInSMJCodegen=true<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.conf.dir=/var/facebook/configerator-client/standalone_configs/datainfra/dw-atn-spark1/<br></span><span class=\"stderr\">Adding default property: spark.scheduler.taskAssigner=packed<br></span><span class=\"stderr\">Adding default property: spark.shuffle.io.numConnectionsPerPeer=4<br></span><span class=\"stderr\">Adding default property: spark.reducer.maxBlocksInFlight=48<br></span><span class=\"stderr\">Adding default property: spark.fb.only.tune.spill.reader.buffer.size.=true<br></span><span class=\"stderr\">Adding default property: spark.remote.io.maxReadBufferSize=4194304<br></span><span class=\"stderr\">Adding default property: spark.speculation.multiplier=4<br></span><span class=\"stderr\">Adding default property: spark.sql.hive.combine.file.input=true<br></span><span class=\"stderr\">Adding default property: spark.default.parallelism=16<br></span><span class=\"stderr\">Adding default property: spark.shuffle.spill.numElementsForceSpillThreshold=33554432<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.codegen.generate.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.perf.agent.path=/packages/java.libperfagent/libperfagent.so<br></span><span class=\"stderr\">Adding default property: spark.history.fs.cleaner.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.dynamicAllocation.maxExecutors=1000<br></span><span class=\"stderr\">Adding default property: spark.hadoop.hive.orc.spectrum.enable=false<br></span><span class=\"stderr\">Adding default property: spark.sql.nestedColumnPruning.multipleExtract.enabled=false<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.substituteMapWithSortedArray=true<br></span><span class=\"stderr\">Adding default property: spark.config.broadwell1.onHeap.size=8g<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.substituteSHJWithSMJNoSort=true<br></span><span class=\"stderr\">Adding default property: spark.sql.orc.vectorized.writer.v1=true<br></span><span class=\"stderr\">Adding default property: spark.fb.only.files.distributeViaGFS=false<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.loadDataWithoutOverwrite.disabled=true<br></span><span class=\"stderr\">Adding default property: spark.remote.io.hconf.ws.cp.connection.pooling=false<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.appMetrics.updater.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.fb.only.transformEnv.NETCLASS_DSCP=10<br></span><span class=\"stderr\">Adding default property: spark.sql.sources.bucketing.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.cosco.chunk.max.decoded.size=301989888<br></span><span class=\"stderr\">Adding default property: spark.sql.sources.broadcastHint.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.ssl.other.enabled=false<br></span><span class=\"stderr\">Adding default property: spark.cosco.uow.send.teardown.on.channel.failure=false<br></span><span class=\"stderr\">Adding default property: spark.hive.optimize.tablesample=true<br></span><span class=\"stderr\">Adding default property: spark.config.memory.override.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.cosco.non.sort.shuffle.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.shuffle.initial.buffer.size=4194304<br></span><span class=\"stderr\">Adding default property: spark.driver.extraJavaOptions=-noverify -Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses=true -Dscala.usejavacp=true -XX:+UseG1GC -XX:+PerfDisableSharedMem -XX:+ExplicitGCInvokesConcurrent -XX:+UseGCOverheadLimit -XX:+PrintGCCause -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -XX:+PrintClassHistogramAfterFullGC -XX:+PrintClassHistogramBeforeFullGC -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCApplicationStoppedTime -XX:+PrintHeapAtGC -Xloggc:/tmp/spark.driver.gc.%p.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=100M -XX:ErrorFile=/tmp/hs_err_pid.%p.log -XX:OnOutOfMemoryError=\"/bin/kill -9 %p\" -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/ -XX:-OmitStackTraceInFastThrow<br></span><span class=\"stderr\">Adding default property: spark.shuffle.merge.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.sql.hive.convertMetastoreOrc=false<br></span><span class=\"stderr\">Adding default property: spark.config.broadwell2.memory.override.enable=true<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.jobStagingPathCleanupEnabled=true<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.application.stats.loggers=org.apache.spark.scheduler.BumblebeeApplicationStatsScubaLogger<br></span><span class=\"stderr\">Adding default property: spark.ws.cp.connection.pooling=false<br></span><span class=\"stderr\">Adding default property: spark.hadoop.mapred.autosplit.minimum.split.size=268435456<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.dynamicAllocation.watcher.enable.kill=true<br></span><span class=\"stderr\">Adding default property: spark.ext.h2o.disable.ga=true<br></span><span class=\"stderr\">Adding default property: spark.config.broadwell2.offHeap.size=5g<br></span><span class=\"stderr\">Adding default property: spark.config.skylake.onHeap.size=6g<br></span><span class=\"stderr\">Adding default property: spark.shuffle.merge.merger.maxMergeNum=20<br></span><span class=\"stderr\">Adding default property: spark.executor.heartbeat.maxFailures=15<br></span><span class=\"stderr\">Adding default property: spark.sql.broadcastTimeout=86400<br></span><span class=\"stderr\">Adding default property: spark.hlt.logger.incCap.coefficient=1.7<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.kill.on.uncaught.exception=false<br></span><span class=\"stderr\">Adding default property: spark.fb.only.transformEnv.OMP_NUM_THREADS=1<br></span><span class=\"stderr\">Adding default property: spark.fb.only.transformEnv.LD_PRELOAD=/usr/local/lib/netclass.so<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.fail.executor.on.oom=false<br></span><span class=\"stderr\">Adding default property: spark.statistics.spectrum.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.master=bumblebee://confdir:/var/facebook/configerator-client/standalone_configs/datainfra/dw-atn-spark1<br></span><span class=\"stderr\">Adding default property: spark.sql.orc.vectorized.writer.v1.complex.type=true<br></span><span class=\"stderr\">Adding default property: spark.statistics.spectrum.limitFiles=200000<br></span><span class=\"stderr\">Adding default property: spark.shuffle.blockTransferService=netty<br></span><span class=\"stderr\">Adding default property: spark.shuffle.sort.initialBufferSize=4194304<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.executor.home=fblearner.plasma.tar.gz<br></span><span class=\"stderr\">Adding default property: spark.shuffle.io.essConnectionTimeout=60<br></span><span class=\"stderr\">Adding default property: spark.ssl.historyServer.keyStorePassword=password<br></span><span class=\"stderr\">Adding default property: spark.metrics.conf.executor.sink.scuba.class=org.apache.spark.metrics.sink.SparkExecutorMetricsToScubaSink<br></span><span class=\"stderr\">Adding default property: spark.rpc.message.maxSize=1024<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.ensureSorting=true<br></span><span class=\"stderr\">Adding default property: spark.fb.only.enableExecutorToShuffleServiceRegistration=false<br></span><span class=\"stderr\">Adding default property: spark.xdb.logging.prod.enabled=false<br></span><span class=\"stderr\">Adding default property: spark.sql.enable.dynamic.join.instrumentation=false<br></span><span class=\"stderr\">Adding default property: spark.history.fs.cleaner.interval=1d<br></span><span class=\"stderr\">Adding default property: spark.executorEnv.JAVA_HOME=/usr/local/java-runtime/8-platform007<br></span><span class=\"stderr\">Adding default property: spark.fb.only.sql.insertWithoutOverwrite.disabled=true<br></span><span class=\"stderr\">Adding default property: spark.history.fs.cleaner.maxAge=14d<br></span><span class=\"stderr\">Adding default property: spark.query.latency.minimumLatencyThreshold=4h<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.hiveCastRedundantNullCheckElimination=false<br></span><span class=\"stderr\">Adding default property: spark.remote.io.retry.maxRetryCount=4<br></span><span class=\"stderr\">Adding default property: spark.remote.io.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.executor.extraLibraryPath=fblearner.plasma.tar.gz/lib/ext:/packages/spark.mkl:/packages/java/oracle-jdk/jre/lib/amd64/server<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.comparison.strict=true<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.jstack.custom.command=BUMBLEBEE_PID=$(for i in `cat $(mount | grep cgroup2 | cut -d\" \" -f3)/$(cat /proc/self/cgroup  | grep 0:: | cut -d: -f3)/cgroup.procs`; do echo \"$i:$(cat /proc/$i/cmdline)\" ; echo ; done | grep java | grep fblearner.plasma | head -1 | cut -d: -f1); /usr/local/java-runtime/impl/11/bin/jstack $BUMBLEBEE_PID | /packages/datainfra-bumblebee/current/fb-private/jstack_scuba_uploader.sh --job_id $BUMBLEBEE_SESSION_ID --task_id $BUMBLEBEE_RESOURCE_ID --scribe_category perfpipe_bumblebee_job_perf_samples --thread_name_regex ^Executor.*;<br></span><span class=\"stderr\">Adding default property: spark.ws.chunking.thrift.rollout.percent.OVERRIDE=100<br></span><span class=\"stderr\">Adding default property: spark.executor.memory=8192m<br></span><span class=\"stderr\">Adding default property: spark.cosco.kds.runtime.admission.threshold.ms=10800000<br></span><span class=\"stderr\">Adding default property: spark.fetch.failure.resubmit.timeout=3m<br></span><span class=\"stderr\">Adding default property: spark.shuffle.file.buffer=32m<br></span><span class=\"stderr\">Adding default property: spark.driver.extraClassPath=/usr/local/spark/lib/ext/*<br></span><span class=\"stderr\">Adding default property: spark.fb.only.default.executor.heap.memory=8192m<br></span><span class=\"stderr\">Adding default property: spark.fb.only.enableExternalUnsafeRowLinkedList=true<br></span><span class=\"stderr\">Adding default property: spark.reducer.maxReqsInFlight=32<br></span><span class=\"stderr\">Adding default property: spark.eventLog.dir=ws://ws.dw.atn3oxygen/user/hadoop/dw-atn-spark1/spark/logs<br></span><span class=\"stderr\">Adding default property: spark.ssl.protocol=TLSv1.2<br></span><span class=\"stderr\">Adding default property: spark.ssl.historyServer.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.fb.only.nondeterministic.safe.udf.classes=UDFArrayShuffle,UDFEvalF<br></span><span class=\"stderr\">Adding default property: spark.ssl.historyServer.keyStore=/var/facebook/x509_identities/keystore.jks<br></span><span class=\"stderr\">Adding default property: spark.dynamicAllocation.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.executor.extraClassPath=fblearner.plasma.tar.gz/lib/ext/*:fblearner.plasma.tar.gz/jars/*:/var/facebook/configerator-client/standalone_configs/datainfra/dw-atn-spark1/spark/executor/:hive.udfs/*:application_jar/*:*<br></span><span class=\"stderr\">Adding default property: spark.stage.maxConsecutiveAttempts=20<br></span><span class=\"stderr\">Adding default property: spark.history.based.tuning.kds.maxJobsToLookAt=20<br></span><span class=\"stderr\">Adding default property: spark.sql.nestedColumnPruning.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.rm.enable.session.resync=true<br></span><span class=\"stderr\">Adding default property: spark.cosco.task.use.new.package.id=true<br></span><span class=\"stderr\">Adding default property: spark.eventLog.compress=false<br></span><span class=\"stderr\">Adding default property: spark.fb.only.label.based.scheduling.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.executor.cores=4<br></span><span class=\"stderr\">Adding default property: spark.shuffle.merge.merger.minMergeNum=3<br></span><span class=\"stderr\">Adding default property: spark.history.ui.port=8088<br></span><span class=\"stderr\">Adding default property: spark.fb.only.removeExecutorOnRPCDisconnect=false<br></span><span class=\"stderr\">Adding default property: spark.fb.only.whitelisted.properties=hive.exec.dynamic.partition.mode<br></span><span class=\"stderr\">Adding default property: spark.hadoop.ignition.combine.format.version=size-balanced-split<br></span><span class=\"stderr\">Adding default property: spark.sql.statistics.fallBackToHdfs=true<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.url.loggers=org.apache.spark.scheduler.cluster.BumblebeeHistoryServerUrlLogger,org.apache.spark.scheduler.cluster.BumblebeeScubaUrlLogger,org.apache.spark.scheduler.cluster.BumblebeePerfUrlLogger,org.apache.spark.scheduler.cluster.ChronosJobInstanceLinkLogger,org.apache.spark.scheduler.cluster.SparkUISilfraUrlLogger,org.apache.spark.scheduler.cluster.BumblebeeSessionUrlLogger<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.applicationStats.updater.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.shuffle.file.max.total.buffer=512m<br></span><span class=\"stderr\">Adding default property: spark.cosco.enable.kds=true<br></span><span class=\"stderr\">Adding default property: spark.memory.fraction=0.2<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.enforceHiveHashForBucketing=true<br></span><span class=\"stderr\">Adding default property: spark.ssl.historyServer.protocol=TLSv1.2<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.failQueryWithoutPartitionPredicates.enabled=false<br></span><span class=\"stderr\">Adding default property: spark.ws.use.streaming.read.OVERRIDE=false<br></span><span class=\"stderr\">Adding default property: spark.scheduler.listenerbus.eventqueue.size=20000<br></span><span class=\"stderr\">Adding default property: spark.executorEnv.FB_HIVE_UDF_INSTRUMENTATION_ENABLED=false<br></span><span class=\"stderr\">Adding default property: spark.ssl.trustStorePassword=password<br></span><span class=\"stderr\">Adding default property: spark.dynamicAllocation.minExecutors=1<br></span><span class=\"stderr\">Adding default property: spark.reducer.maxReqSizeShuffleToMem=1g<br></span><span class=\"stderr\">Adding default property: spark.dynamicAllocation.initialExecutors=1<br></span><span class=\"stderr\">Adding default property: spark.fs.api.timeout.enabled=false<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.sqlexception.zero_input_partition.throw=true<br></span><span class=\"stderr\">Adding default property: spark.depa.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.files.useFetchCache=true<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.rm.session.url.format=https://our.intern.facebook.com/intern/bumblebee_proxy/?proxy_url=%s/resourceManager/session.html?sessionId=%d<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.usePlasmaFileOutputCommitter=true<br></span><span class=\"stderr\">Adding default property: spark.security.access.token.prefetch.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.memory.offHeap.size.override=9g<br></span><span class=\"stderr\">Adding default property: spark.shuffle.merge.manager=org.apache.spark.scheduler.ExternalShuffleMergeManager<br></span><span class=\"stderr\">Adding default property: spark.cosco.allocation.compacted=true<br></span><span class=\"stderr\">Adding default property: spark.sql.shuffle.partition.multiplier=0.125<br></span><span class=\"stderr\">Adding default property: spark.hadoop.hive.fb.only.prefer.orc=true<br></span><span class=\"stderr\">Adding default property: spark.ws.shim.enabled=true<br></span><span class=\"stderr\">Adding default property: spark.dynamicAllocation.executorIdleTimeout=10s<br></span><span class=\"stderr\">Adding default property: spark.remote.io.rootDir=tempfs://ws.dw.atn3oxygen/spark<br></span><span class=\"stderr\">Adding default property: spark.core.connection.ack.wait.timeout=600<br></span><span class=\"stderr\">Adding default property: spark.sql.cache.exchange=false<br></span><span class=\"stderr\">Adding default property: spark.sql.fb.only.failOnBucketingValidation=true<br></span><span class=\"stderr\">Adding default property: spark.shuffle.consolidateFiles=true<br></span><span class=\"stderr\">Adding default property: spark.fb.only.fail.executor.on.oom=true<br></span><span class=\"stderr\">Adding default property: spark.fb.only.chainedIteratorMemoryOptimizationEnabled=true<br></span><span class=\"stderr\">Adding default property: spark.remote.io.hconf.ws.tempfs.thrift.chunking.version.OVERRIDE=1<br></span><span class=\"stderr\">Adding default property: spark.bumblebee.perf.custom.command=/usr/bin/perf record -F 1000 -g -p $BUMBLEBEE_PID -o /tmp/perf.$BUMBLEBEE_PID.data -- /bin/usleep 10000 &amp;&amp; /usr/bin/perf script -i /tmp/perf.$BUMBLEBEE_PID.data | /packages/datainfra-bumblebee/current/fb-private/stackcollapse-perf.pl --scuba_output --jobid $BUMBLEBEE_SESSION_ID --taskid $BUMBLEBEE_RESOURCE_ID --scribe_category perfpipe_bumblebee_job_perf_samples;<br></span><span class=\"stderr\">Adding default property: spark.hadoop.mapred.enable.autosplit=true<br></span><span class=\"stderr\">Adding default property: spark.sql.shuffle.partitions.min=1000<br></span><span class=\"stderr\">Adding default property: spark.memory.useLegacyMode=true<br></span><span class=\"stderr\">Adding default property: spark.statistics.spectrum.timeoutMs=600000<br></span><span class=\"stderr\">Adding default property: spark.speculation.quantile=0.95<br></span><span class=\"stderr\">Adding default property: spark.task.maxFailures=8<br></span><span class=\"stderr\">Adding default property: spark.cosco.reducer.use.new.merge.file.address.scheme=true<br></span><span class=\"stderr\">Adding default property: spark.sql.shuffle.partitions=997<br></span><span class=\"stderr\">Parsed arguments:<br></span><span class=\"stderr\">  master                  bumblebee://confdir:/var/facebook/configerator-client/standalone_configs/datainfra/dw-atn-spark1<br></span><span class=\"stderr\">  deployMode              null<br></span><span class=\"stderr\">  executorMemory          8192m<br></span><span class=\"stderr\">  executorCores           4<br></span><span class=\"stderr\">  totalExecutorCores      null<br></span><span class=\"stderr\">  propertiesFile          datainfra/dw-atn-spark1/spark/common/spark-defaults.conf<br></span><span class=\"stderr\">  driverMemory            5g<br></span><span class=\"stderr\">  driverCores             null<br></span><span class=\"stderr\">  driverExtraClassPath    /usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/lib/ext/*:/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/jars/*:/var/facebook/configerator-client/standalone_configs/datainfra/dw-atn-spark1:/usr/local/fbprojects/packages/hive.udfs/652/core-udfs-1.0-SNAPSHOT-standalone.jar:spark-internal<br></span><span class=\"stderr\">  driverExtraLibraryPath  /usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/lib/ext/<br></span><span class=\"stderr\">  driverExtraJavaOptions  -noverify -Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses=true -Dscala.usejavacp=true -XX:+UseG1GC -XX:+PerfDisableSharedMem -XX:+ExplicitGCInvokesConcurrent -XX:+UseGCOverheadLimit -XX:+PrintGCCause -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -XX:+PrintClassHistogramAfterFullGC -XX:+PrintClassHistogramBeforeFullGC -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCApplicationStoppedTime -XX:+PrintHeapAtGC -Xloggc:/tmp/spark.driver.gc.%p.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=100M -XX:ErrorFile=/tmp/hs_err_pid.%p.log -XX:OnOutOfMemoryError=\"/bin/kill -9 %p\" -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/ -XX:-OmitStackTraceInFastThrow<br></span><span class=\"stderr\">  supervise               false<br></span><span class=\"stderr\">  queue                   null<br></span><span class=\"stderr\">  numExecutors            null<br></span><span class=\"stderr\">  files                   null<br></span><span class=\"stderr\">  pyFiles                 null<br></span><span class=\"stderr\">  archives                null<br></span><span class=\"stderr\">  mainClass               org.apache.spark.generichql.GenericHQL<br></span><span class=\"stderr\">  primaryResource         spark-internal<br></span><span class=\"stderr\">  name                    pvc.spark.instagram<br></span><span class=\"stderr\">  childArgs               [{\"appname\": \"pvc.spark.instagram\", \"namespace\": \"instagram\", \"queries\": [{\"show\": false, \"splitQueries\": true, \"cache\": false, \"repartition\": 0, \"coalesce\": 0, \"assertCheck\": false, \"alias\": \"\", \"hql\": \"\\n    \\n    CREATE TABLE IF NOT EXISTS tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d (\\n        fips INT, label FLOAT\\n    )\\n    ROW FORMAT DELIMITED\\n    FIELDS TERMINATED BY '\\\\001'\\n    COLLECTION ITEMS TERMINATED BY '\\\\002'\\n    MAP KEYS TERMINATED BY '\\\\003'\\n    LINES TERMINATED BY '\\\\n'\\n    STORED AS TEXTFILE\\n    TBLPROPERTIES ('RETENTION' = '7');\\n\\n    LOAD DATA LOCAL INPATH '/mnt/vol/gfsfblearner-texas2/flow/data/2020-04-23/d0868775-82e9-48d4-99d8-1509d8bf068b'\\n    OVERWRITE INTO TABLE tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;\\n\\n    \\n    CREATE TABLE IF NOT EXISTS tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8 (\\n        fips INT, label FLOAT\\n    )\\n    PARTITIONED BY (ds STRING, number STRING)\\n    \\n    TBLPROPERTIES('RETENTION' = '180');\\n    \\n    \\n    CREATE DEPENDENT TABLE IF NOT EXISTS tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8_signal\\n    REFERS TO TABLE tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8\\n    GROUPED BY (ds);\\n    \\n    \\n    INSERT OVERWRITE TABLE tmp_pvc_2b33cacd_5841_44fa_bd39_65517510b5d8\\n        PARTITION (ds = \\\"2020-04-21\\\", number = \\\"1\\\")\\n        SELECT fips, label FROM tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;;\\n    \\n\\n    DROP TABLE IF EXISTS tmp_pvc_tmp_pvc_2b33cacd_5841_44fa_bd39_6551751b5d8_staging_27662b9c_9369_483f_b435_b6a667612c2d;\\n    \"}]}]<br></span><span class=\"stderr\">  jars                    null<br></span><span class=\"stderr\">  packages                null<br></span><span class=\"stderr\">  packagesExclusions      null<br></span><span class=\"stderr\">  repositories            null<br></span><span class=\"stderr\">  verbose                 true<br></span><span class=\"stderr\"><br></span><span class=\"stderr\">Spark properties used, including those specified through<br></span><span class=\"stderr\"> --conf and those from the properties file datainfra/dw-atn-spark1/spark/common/spark-defaults.conf:<br></span><span class=\"stderr\">  spark.io.compression.codec -&gt; zstd<br></span><span class=\"stderr\">  spark.bumblebee.appMetrics.updater.enabled -&gt; true<br></span><span class=\"stderr\">  spark.bumblebee.application.stats.loggers -&gt; org.apache.spark.scheduler.BumblebeeApplicationStatsScubaLogger<br></span><span class=\"stderr\">  spark.fb.only.chainedIteratorMemoryOptimizationEnabled -&gt; true<br></span><span class=\"stderr\">  spark.fb.only.transformEnv.NETCLASS_DSCP -&gt; 10<br></span><span class=\"stderr\">  spark.query.latency.minimumLatencyThreshold -&gt; 4h<br></span><span class=\"stderr\">  spark.bumblebee.executor.resource.tuning.enabled.status -&gt; enabled<br></span><span class=\"stderr\">  spark.bumblebee.kill.on.uncaught.exception -&gt; false<br></span><span class=\"stderr\">  spark.fb.only.label.based.scheduling.force.dc -&gt; platform:atn3,instagram:atn3,groups:atn3<br></span><span class=\"stderr\">  spark.sql.orc.vectorized.writer.v1.complex.type -&gt; true<br></span><span class=\"stderr\">  spark.shuffle.blockTransferService -&gt; netty<br></span><span class=\"stderr\">  spark.xdb.logging.prod.enabled -&gt; false<br></span><span class=\"stderr\">  spark.bumblebee.perf.agent.path -&gt; /packages/java.libperfagent/libperfagent.so<br></span><span class=\"stderr\">  spark.bumblebee.tags -&gt; xregion_wrapper_enabled_encryption<br></span><span class=\"stderr\">  spark.sql.switch.join -&gt; true<br></span><span class=\"stderr\">  spark.cosco.enabled -&gt; true<br></span><span class=\"stderr\">  spark.config.broadwell2.onHeap.size -&gt; 8g<br></span><span class=\"stderr\">  spark.fb.only.heterogeneous.hardware.enabled -&gt; true<br></span><span class=\"stderr\">  spark.shuffle.cosco.sorted.shuffle.max.num.shuffles -&gt; 4<br></span><span class=\"stderr\">  spark.ssl.rpc.enabled -&gt; true<br></span><span class=\"stderr\">  spark.sql.dynamicJoin -&gt; true<br></span><span class=\"stderr\">  spark.config.broadwell2.offHeap.size.override -&gt; 9g<br></span><span class=\"stderr\">  spark.shuffle.manager -&gt; org.apache.spark.shuffle.cosco.CoscoShuffleManager<br></span><span class=\"stderr\">  spark.remote.io.hconf.ws.use.streaming.read.OVERRIDE -&gt; true<br></span><span class=\"stderr\">  spark.ssl.historyServer.keyStorePassword -&gt; password<br></span><span class=\"stderr\">  spark.shuffle.sort.bypassMergeThreshold -&gt; 10<br></span><span class=\"stderr\">  spark.fb.only.nssr4.enabled -&gt; true<br></span><span class=\"stderr\">  spark.config.broadwell2.offHeap.size -&gt; 5g<br></span><span class=\"stderr\">  spark.sql.enable.autopartition -&gt; true<br></span><span class=\"stderr\">  spark.rpc.io.backLog -&gt; 8192<br></span><span class=\"stderr\">  spark.cosco.writer.netty.package.transfer.timeout.ms -&gt; 55000<br></span><span class=\"stderr\">  spark.sql.autoShuffledHashJoinThreshold -&gt; 67108864<br></span><span class=\"stderr\">  spark.remote.io.rootDir -&gt; tempfs://ws.dw.atn3oxygen/spark<br></span><span class=\"stderr\">  spark.history.fs.cleaner.enabled -&gt; true<br></span><span class=\"stderr\">  spark.bumblebee.dynamicAllocation.watcher.enabled -&gt; true<br></span><span class=\"stderr\">  spark.sql.fb.only.native.udfs.use.custom -&gt; if,collect_set<br></span><span class=\"stderr\">  spark.fb.only.transform.memory.limit -&gt; 13g<br></span><span class=\"stderr\">  spark.bumblebee.local.mode.dataswarm.operators -&gt; DynaswarmHive2MySQLOperator,HDFS2HiveOperator,Hive2CompoundEyeOperator,Hive2HDFSOperator,HiveTableDifferenceOperator2,MySQL2HiveOperator,ODS2HiveOperator,PrecompPublishPartition2MySQLOperator,Rapido2HiveOperator,Salesforce2HiveOperator,Scuba2HiveOperator,ScubaJoin2HiveOperator,SQLServer2HiveOperator,HivePublishSignalTableOperator,ROperator,LlamasSkewHiveOp,HivePartitionSizeCheckOperator,TableGroupHiveQLOperator,HiveRowCountCheckOperator,DynamicPipelineOperator,HiveQLCheckDifferenceOperator<br></span><span class=\"stderr\">  spark.fb.only.whitelisted.properties -&gt; hive.exec.dynamic.partition.mode<br></span><span class=\"stderr\">  spark.sql.fb.only.usePlasmaFileOutputCommitter -&gt; true<br></span><span class=\"stderr\">  spark.executorEnv.BUMBLEBEE_CLUSTER -&gt; dw-atn-spark1<br></span><span class=\"stderr\">  spark.fb.only.config.validator.fail.on.violation -&gt; true<br></span><span class=\"stderr\">  spark.config.broadwell1.offHeap.size.override -&gt; 9g<br></span><span class=\"stderr\">  spark.history.based.tuning.offHeapSize.enabled -&gt; true<br></span><span class=\"stderr\">  spark.shuffle.merge.merger.maxMergeNum -&gt; 20<br></span><span class=\"stderr\">  spark.memory.offHeap.enabled -&gt; true<br></span><span class=\"stderr\">  spark.executor.extraLibraryPath -&gt; fblearner.plasma.tar.gz/lib/ext:/packages/spark.mkl:/usr/local/java-runtime/impl/8/jre/lib/amd64/server<br></span><span class=\"stderr\">  spark.default.parallelism -&gt; 16<br></span><span class=\"stderr\">  spark.cosco.uow.codec -&gt; com.facebook.di.cosco.AircompressorZstdCodec<br></span><span class=\"stderr\">  spark.files.useFetchCache -&gt; true<br></span><span class=\"stderr\">  spark.io.compression.lz4.blockSize -&gt; 512k<br></span><span class=\"stderr\">  spark.shuffle.unsafe.file.output.buffer -&gt; 32m<br></span><span class=\"stderr\">  spark.ssl.other.enabled -&gt; true<br></span><span class=\"stderr\">  spark.fb.only.enableExecutorToShuffleServiceRegistration -&gt; false<br></span><span class=\"stderr\">  spark.remote.io.hconf.tempfs.inputstream.buffer.bytes.OVERRIDE -&gt; 4194304<br></span><span class=\"stderr\">  spark.sql.broadcastTimeout -&gt; 86400<br></span><span class=\"stderr\">  spark.ssl.shuffle.enabled -&gt; true<br></span><span class=\"stderr\">  spark.bumblebee.dist.packages -&gt; fbpkg:///hive.udfs:652#hive.udfs<br></span><span class=\"stderr\">  spark.sql.fb.only.substituteSHJWithSMJNoSort -&gt; true<br></span><span class=\"stderr\">  spark.remote.io.class -&gt; org.apache.hadoop.fs.TempFileSystemShim<br></span><span class=\"stderr\">  spark.sql.shuffle.partition.multiplier -&gt; 0.125<br></span><span class=\"stderr\">  spark.hadoop.mapred.autosplit.input.threshold -&gt; 10995116277760<br></span><span class=\"stderr\">  spark.sql.fb.only.enforceHiveHashForBucketing -&gt; true<br></span><span class=\"stderr\">  spark.reducer.maxReqsInFlight -&gt; 32<br></span><span class=\"stderr\">  spark.driver.memory -&gt; 5g<br></span><span class=\"stderr\">  spark.bumblebee.fail.executor.on.oom -&gt; false<br></span><span class=\"stderr\">  spark.sql.fb.only.substituteMapWithSortedArray -&gt; true<br></span><span class=\"stderr\">  spark.network.timeout -&gt; 900s<br></span><span class=\"stderr\">  spark.executor.memory -&gt; 8192m<br></span><span class=\"stderr\">  spark.remote.io.hconf.tempfs.use.streaming.read.OVERRIDE -&gt; false<br></span><span class=\"stderr\">  spark.skew.detection.median.ratio -&gt; 10<br></span><span class=\"stderr\">  spark.config.broadwell1.onHeap.size.override -&gt; 4g<br></span><span class=\"stderr\">  spark.history.based.tuning.shuffle.partitions.minShufflePartitions -&gt; 200<br></span><span class=\"stderr\">  spark.scheduler.taskAssigner -&gt; packed<br></span><span class=\"stderr\">  spark.sql.fb.only.loadDataWithoutOverwrite.disabled -&gt; true<br></span><span class=\"stderr\">  spark.cosco.task.reducer.0.retry.regex -&gt; .*(loadshedding request|WSE_BLOCK_READ_NOT_INITIATED|WSE_BLOCK_CHUNK_MISSING|WSE_THRIFT_GENERAL_ERROR|WSE_ZDB_RETRYABLE_ERROR|WSE_THROTTLED|InterruptedIOException|WSE_TASK_EXPIRED|Could not acquire mod lock|Could not acquire fetch lock|java.io.IOException: java.lang.InterruptedException).*<br></span><span class=\"stderr\">  spark.ws.dedicated.client.proxy.tier.suffix -&gt; tt_twdi<br></span><span class=\"stderr\">  spark.memory.offHeap.size.override -&gt; 9g<br></span><span class=\"stderr\">  spark.fb.only.transformEnv.LD_PRELOAD -&gt; /usr/local/lib/netclass.so<br></span><span class=\"stderr\">  spark.shuffle.spill.numElementsForceSpillThreshold -&gt; 33554432<br></span><span class=\"stderr\">  spark.bumblebee.rm.session.url.format -&gt; https://our.intern.facebook.com/intern/bumblebee_proxy/?proxy_url=%s/resourceManager/session.html?sessionId=%d<br></span><span class=\"stderr\">  spark.fb.only.jars.distributeViaBumblebee -&gt; true<br></span><span class=\"stderr\">  spark.sql.fb.only.jobStagingPathCleanupEnabled -&gt; true<br></span><span class=\"stderr\">  spark.driver.extraLibraryPath -&gt; /usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/lib/ext/<br></span><span class=\"stderr\">  spark.sql.fb.only.ensureSorting -&gt; true<br></span><span class=\"stderr\">  spark.remote.io.hconf.tempfs.block.size.OVERRIDE -&gt; 25165824<br></span><span class=\"stderr\">  spark.config.broadwell1.offHeap.size -&gt; 4g<br></span><span class=\"stderr\">  spark.config.skylake.onHeap.size.override -&gt; 4g<br></span><span class=\"stderr\">  spark.skew.detection.minSize -&gt; 10737418240<br></span><span class=\"stderr\">  spark.fb.only.task.no.progress.kill.enabled -&gt; true<br></span><span class=\"stderr\">  spark.hlt.logger.enabled -&gt; false<br></span><span class=\"stderr\">  spark.sql.sources.broadcastHint.enabled -&gt; true<br></span><span class=\"stderr\">  spark.sql.adaptive.skewJoin.enabled -&gt; false<br></span><span class=\"stderr\">  spark.rpc.message.maxSize -&gt; 1024<br></span><span class=\"stderr\">  spark.shuffle.merge.merger.blockSizeThreshold -&gt; 4m<br></span><span class=\"stderr\">  spark.ui.showConsoleProgress -&gt; true<br></span><span class=\"stderr\">  spark.config.broadwell1.onHeap.size -&gt; 8g<br></span><span class=\"stderr\">  spark.eventLog.compress -&gt; false<br></span><span class=\"stderr\">  spark.shuffle.io.retryWait -&gt; 20s<br></span><span class=\"stderr\">  spark.eventLog.enabled -&gt; true<br></span><span class=\"stderr\">  spark.remote.io.hconf.ws.cp.connection.pooling -&gt; false<br></span><span class=\"stderr\">  spark.shuffle.io.numConnectionsPerPeer -&gt; 4<br></span><span class=\"stderr\">  spark.fb.only.mimic.fbhive.outer.join -&gt; true<br></span><span class=\"stderr\">  spark.config.memory.override.enabled -&gt; true<br></span><span class=\"stderr\">  spark.fb.only.aliasAgnosticProjectPartitioning -&gt; true<br></span><span class=\"stderr\">  spark.hadoop.dfs.fastcopy.thread.pool.size -&gt; 64<br></span><span class=\"stderr\">  spark.bumblebee.local.mode.override.enabled -&gt; true<br></span><span class=\"stderr\">  spark.statistics.spectrum.enabled -&gt; true<br></span><span class=\"stderr\">  spark.files.fetchFailure.unRegisterOutputOnHost -&gt; true<br></span><span class=\"stderr\">  spark.bumblebee.executor.resource.tuning.cpu.maximum.fraction -&gt; 0.825<br></span><span class=\"stderr\">  spark.history.ui.port -&gt; 8088<br></span><span class=\"stderr\">  spark.ssl.historyServer.keyStore -&gt; /var/facebook/x509_identities/keystore.jks<br></span><span class=\"stderr\">  spark.remote.io.hconf.ws.tempfs.thrift.chunking.version.OVERRIDE -&gt; 1<br></span><span class=\"stderr\">  spark.history.fs.cleaner.maxAge -&gt; 14d<br></span><span class=\"stderr\">  spark.remote.io.hconf.ws.user.name -&gt; spark<br></span><span class=\"stderr\">  spark.hadoop.ignition.combine.format.version -&gt; size-balanced-split<br></span><span class=\"stderr\">  spark.config.broadwell1.memory.override.enable -&gt; true<br></span><span class=\"stderr\">  spark.bumblebee.jstack.custom.command -&gt; BUMBLEBEE_PID=$(for i in `cat $(mount | grep cgroup2 | cut -d\" \" -f3)/$(cat /proc/self/cgroup  | grep 0:: | cut -d: -f3)/cgroup.procs`; do echo \"$i:$(cat /proc/$i/cmdline)\" ; echo ; done | grep java | grep fblearner.plasma | head -1 | cut -d: -f1); /usr/local/java-runtime/impl/11/bin/jstack $BUMBLEBEE_PID | /packages/datainfra-bumblebee/current/fb-private/jstack_scuba_uploader.sh --job_id $BUMBLEBEE_SESSION_ID --task_id $BUMBLEBEE_RESOURCE_ID --scribe_category perfpipe_bumblebee_job_perf_samples --thread_name_regex ^Executor.*;<br></span><span class=\"stderr\">  spark.fb.only.externalAppendOnlyUnsafeRowArrayLogStats -&gt; false<br></span><span class=\"stderr\">  spark.executorEnv.JAVA_HOME -&gt; /usr/local/java-runtime/8-platform007<br></span><span class=\"stderr\">  spark.sql.statistics.fallBackToHdfs -&gt; true<br></span><span class=\"stderr\">  spark.shuffle.merge.merger.optionalMergeThreshold -&gt; 0.95<br></span><span class=\"stderr\">  spark.sql.fb.only.isParallelWritersForDynamicPart -&gt; false<br></span><span class=\"stderr\">  spark.speculation.multiplier -&gt; 4<br></span><span class=\"stderr\">  spark.shuffle.sort.initialBufferSize -&gt; 4194304<br></span><span class=\"stderr\">  spark.fb.only.default.executor.heap.memory -&gt; 8192m<br></span><span class=\"stderr\">  spark.executorEnv.FB_HIVE_UDF_LOGGING_ENABLED -&gt; false<br></span><span class=\"stderr\">  spark.shuffle.merge.merger -&gt; org.apache.spark.scheduler.FixedBlockSizeMerger<br></span><span class=\"stderr\">  spark.history.based.tuning.shuffle.partitions.maxShufflePartitions -&gt; 6000<br></span><span class=\"stderr\">  spark.config.skylake.offHeap.size.override -&gt; 6g<br></span><span class=\"stderr\">  spark.sql.orc.vectorized.reader.v2 -&gt; true<br></span><span class=\"stderr\">  spark.sql.orc.vectorized.writer.v1 -&gt; true<br></span><span class=\"stderr\">  spark.history.based.tuning.shuffle.partitions.override.enabled -&gt; true<br></span><span class=\"stderr\">  spark.ext.h2o.client.web.port -&gt; 8089<br></span><span class=\"stderr\">  spark.sql.fb.only.generateJoinRowInSMJCodegen -&gt; true<br></span><span class=\"stderr\">  spark.cosco.enable.reducer.task.upper.count -&gt; 6000<br></span><span class=\"stderr\">  spark.fb.only.perOperatorMemoryLimitEnabled -&gt; true<br></span><span class=\"stderr\">  spark.fb.only.transformEnv.OMP_NUM_THREADS -&gt; 1<br></span><span class=\"stderr\">  spark.fb.only.useColumnStats -&gt; false<br></span><span class=\"stderr\">  spark.sql.fb.only.codegen.generate.enabled -&gt; true<br></span><span class=\"stderr\">  spark.hadoop.mapred.enable.autosplit -&gt; true<br></span><span class=\"stderr\">  spark.hbt.use.xdb.for.iskds -&gt; true<br></span><span class=\"stderr\">  spark.cosco.allocation.compacted -&gt; true<br></span><span class=\"stderr\">  spark.fb.only.externalUnsafeRowLinkedListLogStats -&gt; true<br></span><span class=\"stderr\">  spark.speculation.quantile -&gt; 0.95<br></span><span class=\"stderr\">  spark.executorEnv.FB_HIVE_UDF_INSTRUMENTATION_ENABLED -&gt; false<br></span><span class=\"stderr\">  spark.kill.tasks.on.fetch.failure -&gt; true<br></span><span class=\"stderr\">  spark.fb.only.files.distributeViaGFS -&gt; true<br></span><span class=\"stderr\">  spark.sql.join.preferSortMergeJoin -&gt; false<br></span><span class=\"stderr\">  spark.fb.only.nondeterministic.safe.udf.classes -&gt; UDFArrayShuffle,UDFEvalF<br></span><span class=\"stderr\">  spark.ui.port -&gt; 8087<br></span><span class=\"stderr\">  spark.cosco.admission.enabled -&gt; true<br></span><span class=\"stderr\">  spark.shuffle.merge.merger.minMergeNum -&gt; 3<br></span><span class=\"stderr\">  spark.hadoop.mapred.max.num.blocks.per.split -&gt; 2000<br></span><span class=\"stderr\">  spark.fb.only.constant.io.skip.extra.udfs -&gt; fb_map_key_apply,fb_accumulate_by_key<br></span><span class=\"stderr\">  spark.config.skylake.memory.override.enable -&gt; true<br></span><span class=\"stderr\">  spark.hadoop.hive.fb.only.prefer.orc -&gt; true<br></span><span class=\"stderr\">  spark.sql.fb.only.hiveCastRedundantNullCheckElimination -&gt; false<br></span><span class=\"stderr\">  spark.sql.shuffle.partitions.max -&gt; 10000<br></span><span class=\"stderr\">  spark.sql.estimate.stats -&gt; true<br></span><span class=\"stderr\">  spark.files.fetchCachePath -&gt; /var/spark<br></span><span class=\"stderr\">  spark.statistics.spectrum.timeoutMs -&gt; 600000<br></span><span class=\"stderr\">  spark.cosco.netty.connection.retry.number -&gt; 64<br></span><span class=\"stderr\">  spark.bumblebee.jstack.enabled -&gt; true<br></span><span class=\"stderr\">  spark.shuffle.consolidateFiles -&gt; true<br></span><span class=\"stderr\">  spark.io.compression.zstd.bufferSize -&gt; 512k<br></span><span class=\"stderr\">  spark.rpc.io.serverThreads -&gt; 64<br></span><span class=\"stderr\">  spark.sql.sources.bucketing.enabled -&gt; true<br></span><span class=\"stderr\">  spark.serializer -&gt; org.apache.spark.serializer.KryoSerializer<br></span><span class=\"stderr\">  spark.dynamicAllocation.maxExecutors -&gt; 1000<br></span><span class=\"stderr\">  spark.hadoop.hive.orc.spectrum.enable -&gt; false<br></span><span class=\"stderr\">  spark.ws.use.streaming.read.OVERRIDE -&gt; false<br></span><span class=\"stderr\">  spark.remote.io.hconf.tempfs.thompson.max.async.puts.OVERRIDE -&gt; 10<br></span><span class=\"stderr\">  spark.history.based.tuning.shuffle.targetPostShuffleInputSize -&gt; 900000000<br></span><span class=\"stderr\">  spark.ssl.historyServer.protocol -&gt; TLSv1.2<br></span><span class=\"stderr\">  spark.cosco.reducer.use.new.merge.file.address.scheme -&gt; true<br></span><span class=\"stderr\">  spark.executor.extraJavaOptions -&gt; -noverify -Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses=true -Dscala.usejavacp=true -Djava.security.egd=file:///dev/urandom -XX:+PreserveFramePointer -XX:+UnlockDiagnosticVMOptions -XX:+DebugNonSafepoints -XX:FreqInlineSize=128 -XX:+UnlockExperimentalVMOptions -XX:ParallelGCThreads=4 -XX:+UseParallelGC -XX:+UseParallelOldGC -XX:+PrintAdaptiveSizePolicy -XX:+PerfDisableSharedMem -XX:+PrintGCCause -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -XX:+PrintClassHistogramAfterFullGC -XX:+PrintClassHistogramBeforeFullGC -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCApplicationStoppedTime -XX:+PrintHeapAtGC -Xloggc:/tmp/spark.executor.gc.%p.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=20M -XX:ErrorFile=/tmp/hs_err_pid.%p.log -XX:OnOutOfMemoryError=\"echo THIS_IS_JVM_OOM &gt;&amp;2 &amp;&amp; (/bin/kill -9 %p &amp;&amp; echo KILLED &gt;&amp;2) &amp;\" -XX:-OmitStackTraceInFastThrow<br></span><span class=\"stderr\">  spark.speculation -&gt; true<br></span><span class=\"stderr\">  spark.driver.jstack.logging -&gt; false<br></span><span class=\"stderr\">  spark.depa.enabled -&gt; true<br></span><span class=\"stderr\">  spark.cosco.non.sort.shuffle.enabled -&gt; true<br></span><span class=\"stderr\">  spark.remote.io.maxReadBufferSize -&gt; 4194304<br></span><span class=\"stderr\">  spark.history.based.tuning.appNamesToIgnore -&gt; spark-sql,hql_validation_spark,Spark Shell<br></span><span class=\"stderr\">  spark.app.name -&gt; pvc.spark.instagram<br></span><span class=\"stderr\">  spark.config.memory.override.blacklistKds -&gt; true<br></span><span class=\"stderr\">  spark.ws.throttle.response.enabled -&gt; true<br></span><span class=\"stderr\">  spark.hadoop.hive.orc.hll.collection.enabled -&gt; true<br></span><span class=\"stderr\">  spark.sql.hive.convertMetastoreOrc -&gt; false<br></span><span class=\"stderr\">  spark.dynamicAllocation.initialExecutors -&gt; 1<br></span><span class=\"stderr\">  spark.dynamicAllocation.executorIdleTimeout -&gt; 10s<br></span><span class=\"stderr\">  spark.history.fs.logDirectory -&gt; ws://ws.dw.atn3oxygen/user/hadoop/dw-atn-spark1/spark/logs<br></span><span class=\"stderr\">  spark.sql.fb.only.comparison.strict -&gt; true<br></span><span class=\"stderr\">  spark.shuffle.service.enabled -&gt; true<br></span><span class=\"stderr\">  spark.shuffle.merge.enabled -&gt; true<br></span><span class=\"stderr\">  spark.ws.chunking.thrift.rollout.percent.OVERRIDE -&gt; 100<br></span><span class=\"stderr\">  spark.mapred.fs.friendly.bucket.split -&gt; true<br></span><span class=\"stderr\">  spark.dynamicAllocation.minExecutors -&gt; 1<br></span><span class=\"stderr\">  spark.fb.only.tune.spill.reader.buffer.size. -&gt; true<br></span><span class=\"stderr\">  spark.sql.hive.combine.file.input -&gt; true<br></span><span class=\"stderr\">  spark.config.broadwell2.onHeap.size.override -&gt; 4g<br></span><span class=\"stderr\">  spark.fb.only.bypass.nondeterministic.check -&gt; true<br></span><span class=\"stderr\">  spark.shuffle.service.registration.timeout -&gt; 180000<br></span><span class=\"stderr\">  spark.sql.fb.only.codegen.hive.table.scan.enabled -&gt; true<br></span><span class=\"stderr\">  spark.sql.nestedColumnPruning.multipleExtract.enabled -&gt; false<br></span><span class=\"stderr\">  spark.sql.hive.metastorePartitionPruning -&gt; true<br></span><span class=\"stderr\">  spark.config.skylake.onHeap.size -&gt; 6g<br></span><span class=\"stderr\">  spark.memory.offHeap.size -&gt; 4g<br></span><span class=\"stderr\">  spark.sql.fb.only.failQueryWithoutPartitionPredicates.enabled -&gt; false<br></span><span class=\"stderr\">  spark.fb.only.task.no.progress.timeout -&gt; 360m<br></span><span class=\"stderr\">  spark.sql.nestedColumnPruning.enabled -&gt; true<br></span><span class=\"stderr\">  spark.reducer.shuffle.numAlternateAddresses -&gt; 5<br></span><span class=\"stderr\">  spark.config.memory.override.fraction -&gt; 1.0<br></span><span class=\"stderr\">  spark.fb.only.tetris.enabled.status -&gt; logonly<br></span><span class=\"stderr\">  spark.bumblebee.executor.home -&gt; fblearner.plasma.tar.gz<br></span><span class=\"stderr\">  spark.shuffle.merge.merger.fileSizeThreshold -&gt; 5g<br></span><span class=\"stderr\">  spark.sql.fb.only.partition.lease.second -&gt; 86400<br></span><span class=\"stderr\">  spark.hadoop.mapred.autosplit.minimum.split.size -&gt; 268435456<br></span><span class=\"stderr\">  spark.ssl.trustStorePassword -&gt; password<br></span><span class=\"stderr\">  spark.shuffle.io.maxRetries -&gt; 15<br></span><span class=\"stderr\">  spark.hlt.logger.incCap.coefficient -&gt; 1.7<br></span><span class=\"stderr\">  spark.executor.memory.override -&gt; 4g<br></span><span class=\"stderr\">  spark.bumblebee.dynamicAllocation.watcher.enable.kill -&gt; true<br></span><span class=\"stderr\">  spark.memory.useLegacyMode -&gt; true<br></span><span class=\"stderr\">  spark.fb.only.enableExternalUnsafeRowLinkedList -&gt; true<br></span><span class=\"stderr\">  spark.cosco.admission.per.shuffle.enabled -&gt; true<br></span><span class=\"stderr\">  spark.ssl.trustStore -&gt; /etc/pki/java/fb_certs.jks<br></span><span class=\"stderr\">  spark.sql.fb.only.failOnBucketingValidation -&gt; true<br></span><span class=\"stderr\">  spark.driver.extraJavaOptions -&gt; -noverify -Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses=true -Dscala.usejavacp=true -XX:+UseG1GC -XX:+PerfDisableSharedMem -XX:+ExplicitGCInvokesConcurrent -XX:+UseGCOverheadLimit -XX:+PrintGCCause -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -XX:+PrintClassHistogramAfterFullGC -XX:+PrintClassHistogramBeforeFullGC -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCApplicationStoppedTime -XX:+PrintHeapAtGC -Xloggc:/tmp/spark.driver.gc.%p.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=100M -XX:ErrorFile=/tmp/hs_err_pid.%p.log -XX:OnOutOfMemoryError=\"/bin/kill -9 %p\" -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/ -XX:-OmitStackTraceInFastThrow<br></span><span class=\"stderr\">  spark.history.based.tuning.shuffle.partitions.override.fraction -&gt; 1.0<br></span><span class=\"stderr\">  spark.executor.heartbeatInterval -&gt; 60s<br></span><span class=\"stderr\">  spark.config.api.enabled -&gt; true<br></span><span class=\"stderr\">  spark.hive.optimize.tablesample -&gt; true<br></span><span class=\"stderr\">  spark.unsafe.sorter.spill.reader.buffer.size -&gt; 2m<br></span><span class=\"stderr\">  spark.bumblebee.applicationStats.updater.enabled -&gt; true<br></span><span class=\"stderr\">  spark.core.connection.ack.wait.timeout -&gt; 600<br></span><span class=\"stderr\">  spark.fb.only.label.based.scheduling.enabled -&gt; true<br></span><span class=\"stderr\">  spark.task.maxFailures -&gt; 8<br></span><span class=\"stderr\">  spark.ssl.pemFilePath -&gt; /var/facebook/x509_identities/server.pem<br></span><span class=\"stderr\">  spark.bumblebee.queue -&gt; dw-atn-spark1_adhoc<br></span><span class=\"stderr\">  spark.reducer.maxBlocksInFlight -&gt; 48<br></span><span class=\"stderr\">  spark.kryoserializer.buffer.max -&gt; 1g<br></span><span class=\"stderr\">  spark.tetris.router.tier -&gt; tetris.router.prod.spark<br></span><span class=\"stderr\">  spark.fb.only.final.shuffle.partition.override.enabled -&gt; false<br></span><span class=\"stderr\">  spark.ws.user.env -&gt; atnspark1_atn3<br></span><span class=\"stderr\">  spark.remote.io.hconf.dw.security.project -&gt; spark.storage.security<br></span><span class=\"stderr\">  spark.fb.only.sql.insertWithoutOverwrite.disabled -&gt; true<br></span><span class=\"stderr\">  spark.shuffle.merge.manager -&gt; org.apache.spark.scheduler.ExternalShuffleMergeManager<br></span><span class=\"stderr\">  spark.fs.api.timeout.enabled -&gt; false<br></span><span class=\"stderr\">  spark.config.machine.types -&gt; skylake,broadwell1,broadwell2<br></span><span class=\"stderr\">  spark.fb.only.executor.memory.limit -&gt; 24g<br></span><span class=\"stderr\">  spark.fb.only.partial.agg.override.enabled -&gt; true<br></span><span class=\"stderr\">  spark.sql.fb.only.failOnSortingValidation -&gt; true<br></span><span class=\"stderr\">  spark.fb.only.memory.offHeap.size.limit -&gt; 9g<br></span><span class=\"stderr\">  spark.bumblebee.conf.dir -&gt; /var/facebook/configerator-client/standalone_configs/datainfra/dw-atn-spark1/<br></span><span class=\"stderr\">  spark.executorEnv.JAVA_HOME_UNIVERSAL_CLIENT -&gt; /usr/local/fb-jdk-8.102<br></span><span class=\"stderr\">  spark.shuffle.file.max.total.buffer -&gt; 512m<br></span><span class=\"stderr\">  spark.executorEnv.LANG -&gt; en_US.UTF-8<br></span><span class=\"stderr\">  spark.executor.extraClassPath -&gt; fblearner.plasma.tar.gz/lib/ext/*:fblearner.plasma.tar.gz/jars/*:/var/facebook/configerator-client/standalone_configs/datainfra/dw-atn-spark1/spark/executor/:hive.udfs/*:application_jar/*:*<br></span><span class=\"stderr\">  spark.bumblebee.url.loggers -&gt; org.apache.spark.scheduler.cluster.BumblebeeHistoryServerUrlLogger,org.apache.spark.scheduler.cluster.BumblebeeScubaUrlLogger,org.apache.spark.scheduler.cluster.BumblebeePerfUrlLogger,org.apache.spark.scheduler.cluster.ChronosJobInstanceLinkLogger,org.apache.spark.scheduler.cluster.SparkUISilfraUrlLogger,org.apache.spark.scheduler.cluster.BumblebeeSessionUrlLogger<br></span><span class=\"stderr\">  spark.sql.fb.only.sqlexception.zero_input_partition.throw -&gt; true<br></span><span class=\"stderr\">  spark.security.access.token.prefetch.enabled -&gt; true<br></span><span class=\"stderr\">  spark.remote.io.hconf.ws.user.env -&gt; atnspark1_atn3<br></span><span class=\"stderr\">  spark.config.skylake.offHeap.size -&gt; 4g<br></span><span class=\"stderr\">  spark.eventLog.dir -&gt; ws://ws.dw.atn3oxygen/user/hadoop/dw-atn-spark1/spark/logs<br></span><span class=\"stderr\">  spark.fb.only.removeExecutorOnRPCDisconnect -&gt; false<br></span><span class=\"stderr\">  spark.metrics.conf.executor.sink.scuba.class -&gt; org.apache.spark.metrics.sink.SparkExecutorMetricsToScubaSink<br></span><span class=\"stderr\">  spark.config.broadwell2.memory.override.enable -&gt; true<br></span><span class=\"stderr\">  spark.cosco.uow.send.teardown.on.channel.failure -&gt; false<br></span><span class=\"stderr\">  spark.remote.io.enabled -&gt; true<br></span><span class=\"stderr\">  spark.sql.fb.only.ensureBucketing -&gt; true<br></span><span class=\"stderr\">  spark.sql.fb.only.removeSortAfterJoin -&gt; false<br></span><span class=\"stderr\">  spark.sql.fb.only.universalCombineInputFormat -&gt; true<br></span><span class=\"stderr\">  spark.scheduler.listenerbus.eventqueue.size -&gt; 20000<br></span><span class=\"stderr\">  spark.driver.maxResultSize -&gt; 8g<br></span><span class=\"stderr\">  spark.cosco.task.use.new.package.id -&gt; true<br></span><span class=\"stderr\">  spark.extraListeners -&gt; org.apache.spark.scheduler.SparkScubaLogger,org.apache.spark.sql.StagingLocationCleaner<br></span><span class=\"stderr\">  spark.remote.io.hconf.ws.chunking.thrift.rollout.pct.tempfs.OVERRIDE -&gt; 100<br></span><span class=\"stderr\">  spark.ws.user.name -&gt; spark<br></span><span class=\"stderr\">  spark.master -&gt; bumblebee://confdir:/var/facebook/configerator-client/standalone_configs/datainfra/dw-atn-spark1<br></span><span class=\"stderr\">  spark.fb.only.default.executor.offHeap.memory -&gt; 4g<br></span><span class=\"stderr\">  spark.udfs.version -&gt; phase3<br></span><span class=\"stderr\">  spark.broadcast.factory -&gt; org.apache.spark.broadcast.HttpBroadcastFactory<br></span><span class=\"stderr\">  spark.port.maxRetries -&gt; 128<br></span><span class=\"stderr\">  spark.remote.io.shuffle.directFetch -&gt; false<br></span><span class=\"stderr\">  spark.ws.cp.connection.pooling -&gt; false<br></span><span class=\"stderr\">  spark.sql.fb.only.check.permission.output.table -&gt; true<br></span><span class=\"stderr\">  spark.fb.only.maxRowLimit -&gt; 100000<br></span><span class=\"stderr\">  spark.statistics.spectrum.limitFiles -&gt; 200000<br></span><span class=\"stderr\">  spark.sql.cache.exchange -&gt; false<br></span><span class=\"stderr\">  spark.fb.only.fail.executor.on.oom -&gt; true<br></span><span class=\"stderr\">  spark.fb.only.dc.based.executor.resource.configs -&gt; domains/datainfra/spark/atn_spark1_dc_based_resource_configs<br></span><span class=\"stderr\">  spark.stage.maxConsecutiveAttempts -&gt; 20<br></span><span class=\"stderr\">  spark.ssl.protocol -&gt; TLSv1.2<br></span><span class=\"stderr\">  spark.reducer.maxReqSizeShuffleToMem -&gt; 1g<br></span><span class=\"stderr\">  spark.sql.enable.dynamic.join.instrumentation -&gt; false<br></span><span class=\"stderr\">  spark.dynamicAllocation.enabled -&gt; true<br></span><span class=\"stderr\">  spark.fb.only.sql.createFileForEmptyBucket -&gt; true<br></span><span class=\"stderr\">  spark.cosco.enable.kds -&gt; true<br></span><span class=\"stderr\">  spark.memory.fraction -&gt; 0.2<br></span><span class=\"stderr\">  spark.ws.shim.enabled -&gt; true<br></span><span class=\"stderr\">  spark.sql.autoBroadcastJoinThreshold -&gt; 75000000<br></span><span class=\"stderr\">  spark.dynamicAllocation.cachedExecutorIdleTimeout -&gt; 10m<br></span><span class=\"stderr\">  spark.shuffle.initial.buffer.size -&gt; 4194304<br></span><span class=\"stderr\">  spark.sql.parser.quotedRegexColumnNames -&gt; true<br></span><span class=\"stderr\">  spark.shuffle.io.clientThreads -&gt; 4<br></span><span class=\"stderr\">  spark.remote.io.retry.errorCodes -&gt; 5<br></span><span class=\"stderr\">  spark.remote.io.retry.maxRetryCount -&gt; 4<br></span><span class=\"stderr\">  spark.eventLog.buffer.kb -&gt; 1024<br></span><span class=\"stderr\">  spark.blacklist.enabled -&gt; false<br></span><span class=\"stderr\">  spark.config.memory.override.numDaysToLookBack -&gt; 7<br></span><span class=\"stderr\">  spark.io.compression.snappy.blockSize -&gt; 1m<br></span><span class=\"stderr\">  spark.fb.only.transformEnv.MKL_NUM_THREADS -&gt; 1<br></span><span class=\"stderr\">  spark.speculation.interval -&gt; 2m<br></span><span class=\"stderr\">  spark.cosco.task.use.package.id.optimized -&gt; true<br></span><span class=\"stderr\">  spark.hive.outerjoin.supports.filters -&gt; false<br></span><span class=\"stderr\">  spark.sql.fb.only.native.udfs.suppressed -&gt; variance,to_date,fb_array_unique,fb_sum_where,fb_top,max_by,fb_choose_one,fb_collect_map<br></span><span class=\"stderr\">  spark.cosco.chunk.max.decoded.size -&gt; 301989888<br></span><span class=\"stderr\">  spark.ext.h2o.disable.ga -&gt; true<br></span><span class=\"stderr\">  spark.history.based.tuning.kds.maxJobsToLookAt -&gt; 20<br></span><span class=\"stderr\">  spark.shuffle.io.essConnectionTimeout -&gt; 60<br></span><span class=\"stderr\">  spark.executor.cores -&gt; 4<br></span><span class=\"stderr\">  spark.remote.io.hconf.ws.dedicated.client.proxy.tier.suffix -&gt; tt_twdi<br></span><span class=\"stderr\">  spark.hlt.new.logger.enabled -&gt; true<br></span><span class=\"stderr\">  spark.fetch.failure.resubmit.timeout -&gt; 3m<br></span><span class=\"stderr\">  spark.ssl.historyServer.enabled -&gt; true<br></span><span class=\"stderr\">  spark.ws.thrift.chunking.version.OVERRIDE -&gt; 4<br></span><span class=\"stderr\">  spark.shuffle.file.buffer -&gt; 32m<br></span><span class=\"stderr\">  spark.executor.heartbeat.maxFailures -&gt; 15<br></span><span class=\"stderr\">  spark.bumblebee.rm.enable.session.resync -&gt; true<br></span><span class=\"stderr\">  spark.driver.extraClassPath -&gt; /usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/lib/ext/*:/usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/jars/*:/var/facebook/configerator-client/standalone_configs/datainfra/dw-atn-spark1:/usr/local/fbprojects/packages/hive.udfs/652/core-udfs-1.0-SNAPSHOT-standalone.jar:spark-internal<br></span><span class=\"stderr\">  spark.cosco.kds.runtime.admission.threshold.ms -&gt; 10800000<br></span><span class=\"stderr\">  spark.sql.shuffle.partitions.min -&gt; 997<br></span><span class=\"stderr\">  spark.bumblebee.perf.custom.command -&gt; /usr/bin/perf record -F 1000 -g -p $BUMBLEBEE_PID -o /tmp/perf.$BUMBLEBEE_PID.data -- /bin/usleep 10000 &amp;&amp; /usr/bin/perf script -i /tmp/perf.$BUMBLEBEE_PID.data | /packages/datainfra-bumblebee/current/fb-private/stackcollapse-perf.pl --scuba_output --jobid $BUMBLEBEE_SESSION_ID --taskid $BUMBLEBEE_RESOURCE_ID --scribe_category perfpipe_bumblebee_job_perf_samples;<br></span><span class=\"stderr\">  spark.sql.shuffle.partitions -&gt; 997<br></span><span class=\"stderr\">  spark.history.fs.cleaner.interval -&gt; 1d<br></span><span class=\"stderr\">  spark.cosco.task.reducer.1.retry.regex -&gt; .*(loadshedding request|WSE_BLOCK_READ_NOT_INITIATED|WSE_BLOCK_CHUNK_MISSING|WSE_THRIFT_GENERAL_ERROR|WSE_THROTTLED|InterruptedIOException|WSE_TASK_EXPIRED|Could not acquire mod lock|Could not acquire fetch lock|java.io.IOException: java.lang.InterruptedException).*<br></span><span class=\"stderr\">  spark.file.transferTo -&gt; false<br></span><span class=\"stderr\">  spark.ssl.blocktransfer.enabled -&gt; true<br></span><span class=\"stderr\"><br></span><span class=\"stderr\">    <br></span><span class=\"stderr\">20/04/23 07:43:50 INFO ConfigRewriter: Running overrideConfigs<br></span><span class=\"stderr\">20/04/23 07:43:50 WARN ConfigRewriter: Labeling as T1 eligible is disabled because spark.history.based.tuning.t1.datacenterLabel is not set.<br></span><span class=\"stderr\">20/04/23 07:43:50 INFO ConfigRewriter: Getting KDS info for app name: pvc.spark.instagram<br></span><span class=\"stderr\">Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.<br></span><span class=\"stderr\">20/04/23 07:43:51 INFO CertificateFetchTask: Invoking refresh()<br></span><span class=\"stderr\">20/04/23 07:43:51 INFO CertificateFetchTask: Retrieving certificate from storage<br></span><span class=\"stderr\">20/04/23 07:43:51 INFO CertificateFetchTask: Certificate does not need refresh, expiration: <br></span><span class=\"stderr\">20/04/23 07:43:51 INFO CertificateFetchTask: Invoking certificateRefreshed()<br></span><span class=\"stderr\">20/04/23 07:43:51 INFO CertificateFetchTask: Scheduling next run in 86400 seconds<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO PlasmaUtils: Successfully queried past runs for app name: pvc.spark.instagram in 1830ms.<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO ConfigRewriter: isKds(): Number of past runs retrieved = 20, past runs required = 1<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO ConfigRewriter: Querying past failed runs in 7 days for app name: pvc.spark.instagram<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO PlasmaUtils: Successfully queried past runs for app name: pvc.spark.instagram in 36ms.<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO ConfigRewriter: hasPastRunFailed(): Number of past runs retrieved = 5, past runs required = 1<br></span><span class=\"stderr\">20/04/23 07:43:52 WARN ConfigRewriter: Tuning reducers number disabled: job has past failure<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO ShuffleConfigRewriteUtils: Getting historical final shuffle partitions size for app name: pvc.spark.instagram, number of days to look back = 7, max jobs to look at = 3, min jobs to look at = 1, min successful jobs to look at = 1<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO PlasmaUtils: Successfully queried past runs for app name: pvc.spark.instagram in 27ms.<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO ShuffleConfigRewriteUtils: Cannot read json string {}, ignoring<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO ShuffleConfigRewriteUtils: Failed to get enough historical data with runs: List((3377707805055842,0,50285,2), (-1,-1,-1,-1), (1125907987979609,-1,-1,-1)) and number of successful runs: 0<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO ConfigRewriter: Getting KDS info for app name: pvc.spark.instagram<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO ConfigRewriter: isKds(): Number of past runs retrieved = 20, past runs required = 1<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO ConfigRewriter: Querying past failed runs in 7 days for app name: pvc.spark.instagram<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO ConfigRewriter: hasPastRunFailed(): Number of past runs retrieved = 5, past runs required = 1<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO ShuffleConfigRewriteUtils: Do not override final-stage shuffle partitions for appName: pvc.spark.instagram with past failure<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SQLQueriesPrecompiler: Overriding Spark config for queries pre-compilation based on configerator: spark.fb.only.udfs.files.distribution.disabled = true<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SQLQueriesPrecompiler: Overriding Spark config for queries pre-compilation based on configerator: spark.eventLog.enabled = false<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SQLQueriesPrecompiler: Overriding Spark config for queries pre-compilation based on configerator: spark.fb.only.metastore.scuba.logging.disabled = true<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SQLQueriesPrecompiler: Overriding Spark config for queries pre-compilation based on configerator: spark.sql.estimate.stats = false<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SQLQueriesPrecompiler: Overriding Spark config for queries pre-compilation based on configerator: spark.sql.parser.quotedRegexColumnNames = true<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SQLQueriesPrecompiler: Overriding Spark config for queries pre-compilation based on configerator: spark.ui.enabled = false<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SQLQueriesPrecompiler: Overriding Spark config for queries pre-compilation based on configerator: spark.app.name = tetris.inputs.outputs.fetcher<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SQLQueriesPrecompiler: Overriding Spark config for queries pre-compilation based on configerator: spark.sql.hive.metastorePartitionPruning = true<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SQLQueriesPrecompiler: Overriding Spark config for queries pre-compilation based on configerator: spark.statistics.spectrum.enabled = false<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SQLQueriesPrecompiler: Overriding Spark config for queries pre-compilation based on configerator: spark.master = local<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SQLQueriesPrecompiler: Overriding Spark config for queries pre-compilation based on configerator: spark.depa.enabled = false<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SparkContext: Running Spark version 2.0.2290<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SecurityManager: Changing view acls to: jsc<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SecurityManager: Changing modify acls to: jsc<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SecurityManager: Changing view acls groups to: <br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SecurityManager: Changing modify acls groups to: <br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jsc); groups with view permissions: Set(); users  with modify permissions: Set(jsc); groups with modify permissions: Set()<br></span><span class=\"stderr\">20/04/23 07:43:52 WARN SecurityManager: SSLOption Default [SSLOptions{enabled=false, keyStore=None, keyStorePaths=List(), keyStorePassword=None, trustStore=Some(/etc/pki/java/fb_certs.jks), trustStorePaths=ArrayBuffer(/etc/pki/java/fb_certs.jks, /usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/etc/pki/java/fb_certs.jks), trustStorePassword=Some(xxx), protocol=Some(TLSv1.2), enabledAlgorithms=Set()}, pem=Some(/var/facebook/x509_identities/server.pem)]<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO NettyRpcEnv: sslEnabled[true]<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO NettyRpcEnv: sslEnabled[true]<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO Utils: Successfully started service 'sparkDriver' on port 43283.<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SparkEnv: Registering MapOutputTracker<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SparkEnv: Registering BlockManagerMaster<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SecurityManager: Changing view acls to: jsc<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SecurityManager: Changing modify acls to: jsc<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SecurityManager: Changing view acls groups to: <br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SecurityManager: Changing modify acls groups to: <br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jsc); groups with view permissions: Set(); users  with modify permissions: Set(jsc); groups with modify permissions: Set()<br></span><span class=\"stderr\">20/04/23 07:43:52 WARN SecurityManager: SSLOption Default [SSLOptions{enabled=false, keyStore=None, keyStorePaths=List(), keyStorePassword=None, trustStore=Some(/etc/pki/java/fb_certs.jks), trustStorePaths=ArrayBuffer(/etc/pki/java/fb_certs.jks, /usr/local/fbprojects/packages/datainfra/fblearner.plasma/1544/etc/pki/java/fb_certs.jks), trustStorePassword=Some(xxx), protocol=Some(TLSv1.2), enabledAlgorithms=Set()}, pem=Some(/var/facebook/x509_identities/server.pem)]<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SparkEnv: Registering OutputCommitCoordinator<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO TaskAssigner: Constructing TaskAssigner as org.apache.spark.scheduler.PackedAssigner<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO LocalSchedulerBackend: Starting LocalSchedulerBackend for appId = 22505ac1e93046edb86ae80220e1445a<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO Executor: Starting executor ID driver on host localhost<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO LocalSchedulerBackend: Scheduling appmetrics updater.<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO LocalSchedulerBackend: Starting stats consumer in LocalSheduler<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RealTimeStatsUtils: Total Active stages : 0<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RealTimeStatsUtils: Total Active jobs : 0<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: RemoteIO is enabled with config key:spark.remote.io.enabled<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RealTimeStatsCollector: Uncaught exception in thread spark-stats-consumer: null<br></span><span class=\"stderr\">java.lang.NullPointerException<br></span><span class=\"stderr\"> at org.apache.spark.rts.RealTimeStatsUtils$.getAppLevelStats(RealTimeStatsUtils.scala:266)<br></span><span class=\"stderr\"> at org.apache.spark.rts.RealTimeStatsCollector.refreshStats(RealTimeStatsCollector.scala:136)<br></span><span class=\"stderr\"> at org.apache.spark.rts.apps.StatsConsumerRunner$$anon$1.run(StatsConsumer.scala:52)<br></span><span class=\"stderr\"> at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)<br></span><span class=\"stderr\"> at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)<br></span><span class=\"stderr\"> at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)<br></span><span class=\"stderr\"> at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)<br></span><span class=\"stderr\"> at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)<br></span><span class=\"stderr\"> at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)<br></span><span class=\"stderr\"> at java.lang.Thread.run(Thread.java:745)<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: HadoopConf with className:org.apache.hadoop.fs.TempFileSystemShim<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: RootDir:tempfs://ws.dw.atn3oxygen/spark, AppDir:tempfs://ws.dw.atn3oxygen/spark-22505ac1e93046edb86ae80220e1445a, number of subdirectories per directory:64<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: FileRegionBufferLength: 8192<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: RetryInitialWaitTimeMs: 200, MaxRetryCount: 4, RetryableErrorCodes: 5<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: MaxReadBufferSize: 4194304<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: ShuffleDirectFetch: false<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: ProxyFileSystemPool: true<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: Overriding RemoteIO Hadoop conf: ws.dedicated.client.proxy.tier.suffix : tt_twdi<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: Overriding RemoteIO Hadoop conf: tempfs.inputstream.buffer.bytes.OVERRIDE : 4194304<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: Overriding RemoteIO Hadoop conf: tempfs.use.streaming.read.OVERRIDE : false<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: Overriding RemoteIO Hadoop conf: ws.use.streaming.read.OVERRIDE : true<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: Overriding RemoteIO Hadoop conf: ws.user.name : spark<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: Overriding RemoteIO Hadoop conf: dw.security.project : spark.storage.security<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: Overriding RemoteIO Hadoop conf: ws.chunking.thrift.rollout.pct.tempfs.OVERRIDE : 100<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: Overriding RemoteIO Hadoop conf: tempfs.thompson.max.async.puts.OVERRIDE : 10<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: Overriding RemoteIO Hadoop conf: tempfs.block.size.OVERRIDE : 25165824<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: Overriding RemoteIO Hadoop conf: ws.cp.connection.pooling : false<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: Overriding RemoteIO Hadoop conf: ws.user.env : atnspark1_atn3<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO RemoteIOConf: Overriding RemoteIO Hadoop conf: ws.tempfs.thrift.chunking.version.OVERRIDE : 1<br></span><span class=\"stderr\">20/04/23 07:43:52 WARN RemoteIOConf: HIVE_QUERY_SOURCE_JSON env variable is empty.<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO ExternalShuffleMergeManager: Initialized shuffle client on driver, appId=22505ac1e93046edb86ae80220e1445a<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO NettyBlockTransferService: SSL enabled[true]<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43057.<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO NettyBlockTransferService: Server created on devbig323.ftw3.facebook.com:43057<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO log: Logging initialized @4288ms<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SparkContext: Registered listener org.apache.spark.scheduler.SparkScubaLogger<br></span><span class=\"stderr\">20/04/23 07:43:52 INFO SparkContext: Registered listener org.apache.spark.sql.StagingLocationCleaner<br></span><span class=\"stderr\">20/04/23 07:43:53 INFO PlasmaHiveSharedState: Warehouse path is '/data/users/jsc/personal/covid/spark-warehouse'.<br></span><span class=\"stderr\">20/04/23 07:43:53 INFO LocalSchedulerBackend: Logging Basic App Metrics to file.<br></span><span class=\"stderr\">20/04/23 07:43:53 INFO SparkScubaLoggerSingletonHolder$: Initialize spark scuba logger.<br></span><span class=\"stderr\">20/04/23 07:43:54 INFO PlasmaHiveClientImpl: Creating GrokedConfig for cluster default using tags = [default] and name = default<br></span><span class=\"stderr\">20/04/23 07:43:54 INFO AppMetricsScubaSample: Created AppMetricsScubaSample with basic statistics<br></span><span class=\"stderr\">20/04/23 07:43:54 INFO PeriodicAppMetricsUpdater: SPARK_APP_METRICS_PATH = /tmp/tmpcztohdmf/spark_app_metrics.json<br></span><span class=\"stderr\">20/04/23 07:43:55 INFO PlasmaHiveContext: Applying UDFs from Spark common for the base context.<br></span><span class=\"stderr\">20/04/23 07:43:55 INFO PlasmaHiveSharedState: Warehouse path is '/data/users/jsc/personal/covid/spark-warehouse'.<br></span><span class=\"stderr\">20/04/23 07:43:55 INFO NssrMgmt: Properties of namespace instagram: DiNamespace{name=instagram, owner=erichecht, oncall=ig_efficiency_task_force, scope=REGION, deployments={DISASTER=[DiNamespaceLocation{location=DiLocationSpec{warehouseType=PRODUCTION, region=texas, datacenter=null, diCluster=null}}], MASTER=[DiNamespaceLocation{location=DiLocationSpec{warehouseType=PRODUCTION, region=altoona, datacenter=null, diCluster=null}}], CANDIDATE=[DiNamespaceLocation{location=DiLocationSpec{warehouseType=PRODUCTION, region=altoona, datacenter=null, diCluster=null}}, DiNamespaceLocation{location=DiLocationSpec{warehouseType=PRODUCTION, region=texas, datacenter=null, diCluster=null}}]}, metastore=MetastoreNamespaceConfig{xdbShard=xdb.metastore.prod.instagram.93042, dbId=42, smcTier=datainfra.metastore.atn}, replication=ReplicationConfig{xdbShard=xdb.di-replication.cozcb_instagram}, compendium=CompendiumConfig{xdbShard=xdb.compendium.zcdky_instagram}, status=ENABLED, dataswarm=DataswarmConfig{chronosAdminClient=chronos_atn_admin_client, disaster=DataswarmDisasterCfg{enabledTasks=ALL, enabledTools=DataswarmEnabledTools{prod=true, backfill=true, tester=true}}}, chronos=ChronosConfig{chronosAdminClient=chronos_atn_admin_client}, macrosMap={DATABEE=, HIVE_HOME=/usr/local, HDFS_LOCATION_STAGING=ws://ws.dw.atn3oxygen/namespace/instagram/staging, NAMESPACE_WAREHOUSE_PATH=/namespace/instagram/warehouse, HIVE=/usr/local/bin/hive --namespace instagram, HADOOP=/usr/local/bin/hadoop, REGION_NAME=atn3oxygen, HDFS_LOCATION=ws://ws.dw.atn3oxygen/namespace/instagram, HIVE_DATABASE_NAME=instagram, DATABEE_HOME=, HADOOP_HOME=/usr/local, HDFS_LOCATION_NON_HIVE=ws://ws.dw.atn3oxygen/namespace/instagram/nonhive, MAP_REDUCE_POOL_GROUP_NAME=}, customProperties={backfillJobStartTime=1582738470}}<br></span><span class=\"stderr\">20/04/23 07:43:55 INFO NssrMgmt: Region of instagram: atn3oxygen<br></span><span class=\"stderr\">20/04/23 07:43:55 INFO PlasmaHiveClientImpl: Creating GrokedConfig for cluster atn3oxygen using tags = [namespace/instagram,namespace/GLOBAL,atn3oxygen] and name = atn3oxygen<br></span><span class=\"stderr\">20/04/23 07:43:55 INFO NssrMgmt: Region of instagram: atn3oxygen<br></span><span class=\"stderr\">20/04/23 07:43:55 INFO NssrMgmt: DFS location for instagram: ws://ws.dw.atn3oxygen/namespace/instagram<br></span><span class=\"stderr\">20/04/23 07:43:55 INFO NssrMgmt: hive.exec.scratchdir for instagram: ws://ws.dw.atn3oxygen/namespace/instagram/tmp<br></span><span class=\"stderr\">20/04/23 07:43:55 INFO NssrMgmt: DFS location for instagram: ws://ws.dw.atn3oxygen/namespace/instagram<br></span><span class=\"stderr\">20/04/23 07:43:55 INFO NssrMgmt: fs.default.name for instagram: ws://ws.dw.atn3oxygen<br></span><span class=\"stderr\">20/04/23 07:43:55 INFO NssrMgmt: fbhive.metastore.thrift.tier of instagram: datainfra.metastore.atn<br></span><span class=\"stderr\">20/04/23 07:43:55 INFO NssrMgmt: fbhive.metastore.thrift.tier of instagram: datainfra.metastore.atn<br></span><span class=\"stderr\">20/04/23 07:43:56 INFO PlasmaHiveSessionState: Spark-native implementation of variance UDF is disabled, skipping it<br></span><span class=\"stderr\">20/04/23 07:43:56 INFO PlasmaHiveSessionState: Spark-native implementation of fb_top UDF is disabled, skipping it<br></span></div>\n",
       "</div>"
      ],
      "text/plain": [
       "<bento.lib.flow.runner.OperatorArea at 0x7f7c2dd0f650>"
      ]
     },
     "metadata": {
      "application/json": {
       "neon": {
        "id": "610b5f1e-8b8b-4ad7-9eb3-30d81adc68b6",
        "type": "OperatorArea"
       }
      },
      "bento_obj_id": "140171321341520",
      "bento_order": 3,
      "text/html": {
       "bento_materialized_id": "b01e9aa4-57a4-4d46-ac5b-5a665c2c7112"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "upload_labels_df_to_hive(death_labels_df, '2020-04-21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "bento_kernel_default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
